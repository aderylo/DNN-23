{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISUBZbOTVZxn"
      },
      "source": [
        "# Low-shot visual anomaly detection (v2.1)\n",
        "\n",
        "*This is the updated version of the homework with some additional clarifications and changes in the task descriptions (see slack channel for details). The code remains untouched.*\n",
        "\n",
        "In this notebook you'll investigate visual anomaly detection in a typical industrial setting - we don't have much data and we can train only only normal (non-anomalous) examples.\n",
        "Read the [PADIM paper](https://arxiv.org/pdf/2011.08785.pdf) carefully.\n",
        "The code here is based on the original implementation from its authors.\n",
        "\n",
        "If you have any questions - please write them on slack in the channel.\n",
        "\n",
        "### Bibliography\n",
        "\n",
        "1. Defard, T., Setkov, A., Loesch, A., & Audigier, R. (2021). [Padim: a patch distribution modeling framework for anomaly detection and localization](https://arxiv.org/pdf/2011.08785.pdf). In International Conference on Pattern Recognition (pp. 475-489). Cham: Springer International Publishing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Gwf82SgVZxo"
      },
      "source": [
        "## Data\n",
        "\n",
        "In case of any problems - please visit [MVTec AD](https://www.mvtec.com/company/research/datasets/mvtec-ad/downloads) to get the access to the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yRQ3gPIVZxp"
      },
      "outputs": [],
      "source": [
        "# %pip install --quiet gdown  # for those who don't run it on Google Colab\n",
        "# !gdown -q '1r7WJeDb-E5zzgQSOx7F7bNWg8kYX3yKE'\n",
        "# !gdown -q '1Kb420ygkN1iBni5Iy_-psLGNoY0gQFk9'\n",
        "# !gdown -q '12wDP9I3aVIr1qLekWY3GLhQO7c6SRhGn'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "liTm9VH-VZxp"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import tarfile\n",
        "\n",
        "DATA_PATH = Path('./mvtec_anomaly_detection')\n",
        "DATA_PATH.mkdir(exist_ok=True)\n",
        "\n",
        "for class_name in ['bottle', 'metal_nut', 'transistor']:\n",
        "    if not (DATA_PATH / class_name).exists():\n",
        "        with tarfile.open(class_name + '.tar.xz') as tar:\n",
        "            tar.extractall(path=DATA_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQyQYffqVZxq"
      },
      "source": [
        "## PADIM implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uvPuyg5vVZxq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "from random import sample\n",
        "from typing import cast, Any, Dict, List, Optional, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.backends, torch.backends.mps\n",
        "import torch.nn.functional as F\n",
        "from numpy.typing import NDArray\n",
        "from matplotlib import colors\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from skimage import morphology\n",
        "from skimage.segmentation import mark_boundaries\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models import wide_resnet50_2, resnet18, Wide_ResNet50_2_Weights, ResNet18_Weights\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "FloatNDArray = NDArray[np.float32]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SuSoeQJMVZxr"
      },
      "outputs": [],
      "source": [
        "# Leave it as is if you're unsure, this notebook will guess this for you below.\n",
        "DEVICE: Optional[torch.device] = None\n",
        "SEED: int = 42  # do not modify\n",
        "\n",
        "plt.style.use(\"dark_background\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w6q4z2STVZxr"
      },
      "outputs": [],
      "source": [
        "def seed_all(seed: int = 0) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def get_best_device_for_pytorch() -> torch.device:\n",
        "    if torch.cuda.is_available():\n",
        "        device_str = \"cuda\"     # GPU\n",
        "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        device_str = \"mps\"      # Apple silicon\n",
        "    else:\n",
        "        device_str = \"cpu\"      # CPU\n",
        "    return torch.device(device_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "VJUMYdKqVZxr",
        "outputId": "3c7a5fd5-4d39-4520-f9e1-85e11058e48a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using PyTorch with cpu backend.\n",
            "Seeded everything with 42.\n"
          ]
        }
      ],
      "source": [
        "if not DEVICE:\n",
        "    DEVICE = get_best_device_for_pytorch()\n",
        "    \n",
        "print(f\"Using PyTorch with {DEVICE} backend.\")\n",
        "\n",
        "seed_all(SEED)\n",
        "print(f\"Seeded everything with {SEED}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YAV-eWoVZxs"
      },
      "source": [
        "### MVTecDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FFDn_Y7xVZxt"
      },
      "outputs": [],
      "source": [
        "class MVTecDataset(Dataset[Tuple[torch.Tensor, int, torch.Tensor]]):\n",
        "    \"\"\"MVTec dataset of industrial objects with and without anomalies.\n",
        "\n",
        "    Yields (x, y, mask) tuples where:\n",
        "    - x is an RGB image from the class, as float tensor of shape (3, cropsize, cropsize);\n",
        "    - y is an int, 0 for good images, 1 for anomalous images;\n",
        "    - mask is 0 for normal pixels, 1 for anomalous pixels, as float tensor of shape (1, cropsize, cropsize).\n",
        "\n",
        "    Source: https://github.com/xiahaifeng1995/PaDiM-Anomaly-Detection-Localization-master/blob/main/datasets/mvtec.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset_path: Path, class_name: str = 'bottle',\n",
        "                 is_train: bool = True, resize: int = 256, cropsize: int = 224, return_only_indices=False):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.class_name = class_name\n",
        "        assert (dataset_path / class_name).is_dir(), f'Dataset class not found: {dataset_path / class_name}'\n",
        "        self.is_train = is_train\n",
        "\n",
        "        self.resize = resize\n",
        "        self.cropsize = cropsize\n",
        "\n",
        "        # load dataset\n",
        "        self.x, self.y, self.mask = self.load_dataset_folder()\n",
        "\n",
        "        # set transforms\n",
        "        self.transform_x = T.Compose([T.Resize(resize, Image.LANCZOS),\n",
        "                                      T.CenterCrop(cropsize),\n",
        "                                      T.ToTensor(),\n",
        "                                      T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                  std=[0.229, 0.224, 0.225])])\n",
        "        self.transform_mask = T.Compose([T.Resize(resize, Image.NEAREST),\n",
        "                                         T.CenterCrop(cropsize),\n",
        "                                         T.ToTensor()])\n",
        "\n",
        "        self.return_only_indices = return_only_indices\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int, torch.Tensor]:\n",
        "        if self.return_only_indices:  # Used for checking the answer of T1.1.\n",
        "            return idx\n",
        "\n",
        "        x, y, mask = self.x[idx], self.y[idx], self.mask[idx]\n",
        "\n",
        "        x = Image.open(x).convert('RGB')\n",
        "        x = cast(torch.Tensor, self.transform_x(x))\n",
        "\n",
        "        if y == 0:\n",
        "            mask = torch.zeros([1, self.cropsize, self.cropsize])\n",
        "        else:\n",
        "            assert mask is not None\n",
        "            mask = Image.open(mask)\n",
        "            mask = cast(torch.Tensor, self.transform_mask(mask))\n",
        "\n",
        "        return x, y, mask\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.x)\n",
        "\n",
        "    def load_dataset_folder(self) -> Tuple[List[Path], List[int], List[Optional[Path]]]:\n",
        "        phase = 'train' if self.is_train else 'test'\n",
        "        x: List[Path] = []\n",
        "        y: List[int] = []\n",
        "        mask: List[Optional[Path]] = []\n",
        "\n",
        "        img_dir = self.dataset_path / self.class_name / phase\n",
        "        gt_dir = self.dataset_path / self.class_name / 'ground_truth'\n",
        "\n",
        "        for img_type_dir in sorted(img_dir.iterdir()):\n",
        "            # Load images.\n",
        "            if not img_type_dir.is_dir():\n",
        "                continue\n",
        "            img_fpath_list = sorted(img_type_dir.glob('*.png'))\n",
        "            x.extend(img_fpath_list)\n",
        "\n",
        "            # Load ground-truth labels and masks.\n",
        "            if img_type_dir.name == 'good':\n",
        "                y.extend([0] * len(img_fpath_list))\n",
        "                mask.extend([None] * len(img_fpath_list))\n",
        "            else:\n",
        "                y.extend([1] * len(img_fpath_list))\n",
        "                mask.extend([gt_dir / img_type_dir.name / (f.stem + '_mask.png')\n",
        "                            for f in img_fpath_list])\n",
        "\n",
        "        assert len(x) == len(y) == len(mask), 'Number of x, y, and mask should be the same.'\n",
        "        return x, y, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jc3w7-nVVZxt"
      },
      "outputs": [],
      "source": [
        "def sample_idx(number_of_features: int, max_number_of_features: int) -> torch.Tensor:\n",
        "    assert number_of_features <= max_number_of_features\n",
        "    return torch.tensor(sample(range(0, max_number_of_features), number_of_features))\n",
        "\n",
        "\n",
        "def denormalization(x: FloatNDArray) -> NDArray[np.uint8]:\n",
        "    \"\"\"Denormalize with ImageNet values.\"\"\"\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    return (((x.transpose(1, 2, 0) * std) + mean) * 255.).astype(np.uint8)\n",
        "\n",
        "\n",
        "def embedding_concat(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Concatenate embeddings (along the channel dimension, upscaling y to match x).\n",
        "\n",
        "    Args:\n",
        "        x: Tensor of shape (B, C1, H1, W1).\n",
        "        y: Tensor of shape (B, C2, H2, W2).\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape is (B, C1 + C2, H1, W1).\n",
        "    \"\"\"\n",
        "    B, C1, H1, W1 = x.size()\n",
        "    _, C2, H2, W2 = y.size()\n",
        "    s = int(H1 / H2)\n",
        "    x = F.unfold(x, kernel_size=s, dilation=1, stride=s)\n",
        "    x = x.view(B, C1, s * s, H2, W2)\n",
        "    z = torch.zeros(B, C1 + C2, s * s, H2, W2).to(x.device)\n",
        "    for i in range(s * s):\n",
        "        z[:, :, i, :, :] = torch.cat((x[:, :, i, :, :], y), dim=1)\n",
        "    z = z.view(B, -1, H2 * W2)\n",
        "    z = F.fold(z, kernel_size=s, output_size=(H1, W1), stride=s)\n",
        "    return z\n",
        "\n",
        "def concatenate_embeddings_from_all_layers(layer_outputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
        "        embedding_vectors = layer_outputs['layer1']\n",
        "        for layer_name in ['layer2', 'layer3']:\n",
        "            embedding_vectors = embedding_concat(embedding_vectors, layer_outputs[layer_name])\n",
        "        return embedding_vectors\n",
        "\n",
        "def plot_fig(test_img, scores, gts, threshold: float, save_dir: Path, class_name: str):\n",
        "    num = len(scores)\n",
        "    vmax = scores.max() * 255.\n",
        "    vmin = scores.min() * 255.\n",
        "    for i in range(num):\n",
        "        img = test_img[i]\n",
        "        img = denormalization(img)\n",
        "        gt = gts[i].transpose(1, 2, 0).squeeze()\n",
        "        heat_map = scores[i] * 255\n",
        "        mask = scores[i]\n",
        "        mask[mask > threshold] = 1\n",
        "        mask[mask <= threshold] = 0\n",
        "        kernel = morphology.disk(4)\n",
        "        mask = morphology.opening(mask, kernel)\n",
        "        mask *= 255\n",
        "        vis_img = mark_boundaries(img, mask, color=(1, 0, 0), mode='thick')\n",
        "        fig_img, ax_img = plt.subplots(1, 5, figsize=(12, 3))\n",
        "        fig_img.subplots_adjust(right=0.9)\n",
        "        norm = colors.Normalize(vmin=vmin, vmax=vmax)\n",
        "        for ax_i in ax_img:\n",
        "            ax_i.axes.xaxis.set_visible(False)\n",
        "            ax_i.axes.yaxis.set_visible(False)\n",
        "        ax_img[0].imshow(img)\n",
        "        ax_img[0].title.set_text('Image')\n",
        "        ax_img[1].imshow(gt, cmap='gray')\n",
        "        ax_img[1].title.set_text('GroundTruth')\n",
        "        ax = ax_img[2].imshow(heat_map, cmap='jet', norm=norm)\n",
        "        ax_img[2].imshow(img, cmap='gray', interpolation='none')\n",
        "        ax_img[2].imshow(heat_map, cmap='jet', alpha=0.5, interpolation='none')\n",
        "        ax_img[2].title.set_text('Predicted heat map')\n",
        "        ax_img[3].imshow(mask, cmap='gray')\n",
        "        ax_img[3].title.set_text('Predicted mask')\n",
        "        ax_img[4].imshow(vis_img)\n",
        "        ax_img[4].title.set_text('Segmentation result')\n",
        "        left = 0.92\n",
        "        bottom = 0.15\n",
        "        width = 0.015\n",
        "        height = 1 - 2 * bottom\n",
        "        rect = [left, bottom, width, height]\n",
        "        cbar_ax = fig_img.add_axes(rect)\n",
        "        cb = plt.colorbar(ax, shrink=0.6, cax=cbar_ax, fraction=0.046)\n",
        "        cb.ax.tick_params(labelsize=8)\n",
        "        font = {\n",
        "            'family': 'serif',\n",
        "            'color': 'black',\n",
        "            'weight': 'normal',\n",
        "            'size': 8,\n",
        "        }\n",
        "        cb.set_label('Anomaly Score', fontdict=font)\n",
        "\n",
        "        fig_img.savefig(save_dir / f'{class_name}_{i}', dpi=100)\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VGs7EmL9VZxt"
      },
      "outputs": [],
      "source": [
        "def get_feature_extractor(arch: str) -> nn.Module:\n",
        "    if arch == 'resnet18':\n",
        "        model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1, progress=True)\n",
        "        # t_d = 448\n",
        "        # d = 40\n",
        "    elif arch == 'wide_resnet50_2':\n",
        "        model = wide_resnet50_2(weights=Wide_ResNet50_2_Weights.IMAGENET1K_V1, progress=True)\n",
        "        # t_d = 1792\n",
        "        # d = 550\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzRgYQYxVZxu"
      },
      "source": [
        "### PADIM class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "m5AzS1u1VZxu"
      },
      "outputs": [],
      "source": [
        "class PADIM():\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            backbone: str,\n",
        "            device: torch.device,\n",
        "            save_path: Path,\n",
        "            backbone_features_idx: torch.Tensor,\n",
        "            class_names: List[str] = [\"bottle\"],\n",
        "            plot_metrics: bool = False,\n",
        "    ) -> None:\n",
        "        self.arch = backbone\n",
        "        self.device = device\n",
        "        self.model = get_feature_extractor(backbone)\n",
        "        self.model.to(device)\n",
        "        self.model.eval()\n",
        "\n",
        "        self.feature_subset_indices = backbone_features_idx\n",
        "        self.feature_subset_indices.to(device)\n",
        "\n",
        "        self.outputs: Dict[str, torch.Tensor] = {}\n",
        "\n",
        "        self.class_names = class_names\n",
        "        self.save_path = save_path\n",
        "        self.plot_metrics = plot_metrics\n",
        "\n",
        "        self.setup_hooks()\n",
        "        (self.save_path / f'temp_{self.arch}').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.mean: FloatNDArray  # shape (C, H * W)\n",
        "        self.cov: FloatNDArray  # shape (C, C, H * W)\n",
        "\n",
        "    def setup_hooks(self):\n",
        "        \"\"\"Setup hooks to store model's intermediate outputs.\"\"\"\n",
        "        self.model.layer1[-1].register_forward_hook(lambda _, __, x: self.outputs.update({'layer1': x}))\n",
        "        self.model.layer2[-1].register_forward_hook(lambda _, __, x: self.outputs.update({'layer2': x}))\n",
        "        self.model.layer3[-1].register_forward_hook(lambda _, __, x: self.outputs.update({'layer3': x}))\n",
        "\n",
        "    def train_and_test(self, train_dataloader: DataLoader, test_dataloader: DataLoader) -> float:\n",
        "        self.train(train_dataloader)\n",
        "        return self.test(test_dataloader)\n",
        "\n",
        "    def train(self, train_dataloader: DataLoader) -> None:\n",
        "        self.train_outputs: Dict[str, List[torch.Tensor]] = {'layer1': [], 'layer2': [], 'layer3': []}\n",
        "        for x, _, _ in tqdm(train_dataloader, desc='Feature extraction (train)'):\n",
        "            # Run model prediction.\n",
        "            with torch.no_grad():\n",
        "                _ = self.model(x.to(DEVICE))\n",
        "            # Get intermediate layer outputs.\n",
        "            assert list(self.outputs.keys())  == ['layer1', 'layer2', 'layer3'], list(self.outputs.keys())\n",
        "            for k, v in self.outputs.items():\n",
        "                self.train_outputs[k].append(v.cpu().detach())\n",
        "            # Reset hook outputs.\n",
        "            self.outputs = {}\n",
        "\n",
        "        embedding_vectors = concatenate_embeddings_from_all_layers(\n",
        "            {k: torch.cat(v, 0) for k, v in self.train_outputs.items()})\n",
        "        embedding_vectors_subset = torch.index_select(embedding_vectors, 1, self.feature_subset_indices.cpu())\n",
        "\n",
        "        self.mean, self.cov = self.estimate_multivariate_gaussian(embedding_vectors_subset)\n",
        "        del(self.train_outputs)\n",
        "\n",
        "    def estimate_multivariate_gaussian(self, embedding_vectors: torch.Tensor\n",
        "                                       ) -> Tuple[FloatNDArray, FloatNDArray]:\n",
        "        \"\"\"Calculates multivariate Gaussian distribution.\n",
        "\n",
        "        Takes embeddings of shape (N, C, H, W).\n",
        "        Returns (mean, covariance) of shape (C, H * W) and (C, C, H * W) respectively.\n",
        "        \"\"\"\n",
        "        B, C, H, W = embedding_vectors.size()\n",
        "        embedding_vectors = embedding_vectors.view(B, C, H * W)\n",
        "        mean = torch.mean(embedding_vectors, dim=0).numpy()\n",
        "        cov = torch.zeros(C, C, H * W).numpy()\n",
        "        I = np.identity(C)\n",
        "        for i in tqdm(range(H * W), desc=\"Covariance estimation\"):\n",
        "            cov[:, :, i] = np.cov(embedding_vectors[:, :, i].numpy(), rowvar=False) + 0.01 * I\n",
        "        return mean, cov\n",
        "\n",
        "    def test(self, test_dataloader: DataLoader) -> float:\n",
        "        self.test_outputs: Dict[str, List[torch.Tensor]] = {'layer1': [], 'layer2': [], 'layer3': []}\n",
        "        test_imgs: List[FloatNDArray] = []\n",
        "        gt_list: List[NDArray[Any]] = []\n",
        "        gt_mask_list: List[FloatNDArray] = []\n",
        "\n",
        "        # Extract test set features.\n",
        "        for x, y, mask in tqdm(test_dataloader, desc='Feature extraction (test)', disable=False):\n",
        "            x_shape = x.shape\n",
        "            test_imgs.extend(x.cpu().detach().numpy())\n",
        "            gt_list.extend(y.cpu().detach().numpy())\n",
        "            gt_mask_list.extend(mask.cpu().detach().numpy())\n",
        "            # Run model prediction.\n",
        "            with torch.no_grad():\n",
        "                _ = self.model(x.to(DEVICE))\n",
        "            # Get intermediate layer outputs.\n",
        "            assert list(self.outputs.keys())  == ['layer1', 'layer2', 'layer3']\n",
        "            for k, v in self.outputs.items():\n",
        "                self.test_outputs[k].append(v.cpu().detach())\n",
        "            # Reset hook outputs.\n",
        "            self.outputs = {}\n",
        "        gt_mask = np.asarray(gt_mask_list)  # shape (len(test_dataset), 1, H, W)\n",
        "\n",
        "        embedding_vectors = concatenate_embeddings_from_all_layers(\n",
        "            {k: torch.cat(v, 0) for k, v in self.test_outputs.items()})\n",
        "        # shape (len(test_dataset), len(feature_subset_indices), H1, W1)\n",
        "        embedding_vectors_subset = torch.index_select(embedding_vectors, 1, self.feature_subset_indices.cpu())\n",
        "\n",
        "        distances = self.calculate_distances(embedding_vectors_subset)\n",
        "        score_map = self.prepare_anomaly_map((x_shape[2], x_shape[3]), distances)\n",
        "\n",
        "        img_fpr, img_tpr, img_auroc = self.calculate_auroc_image_level(score_map, gt_list)\n",
        "        pxl_fpr, pxl_tpr, pxl_auroc = self.calculate_auroc_pixel_level(score_map, gt_mask)\n",
        "\n",
        "        if self.plot_metrics:\n",
        "            print(f'[TEST] Image AUROC: {img_auroc:.3f}')\n",
        "            print(f'[TEST] Pixel AUROC: {pxl_auroc:.3f}')\n",
        "            threshold = self.calculate_optimal_threshold(score_map, gt_mask)\n",
        "            self.plot_test_results_for_class(gt_mask_list, test_imgs, score_map, threshold, img_fpr, img_tpr, img_auroc, pxl_fpr, pxl_tpr, pxl_auroc)\n",
        "\n",
        "        return pxl_auroc\n",
        "\n",
        "    # TODO: Some of your code for Task 1 goes here. You can add more functions if needed, but use the ones below - we will use them for checking your solution.\n",
        "    def test_permutation_importance(self, val_dataloader: DataLoader, features_to_permute: List[int]) -> List[float]:\n",
        "        \"\"\"Runs a series of tests on `val_dataloader`.\n",
        "        Returns a list of pixelwise AUROCs, where the n-th element of the list is generated by testing the embeddings from `permute_feature(embeddings, features_to_permute[n]).\"\"\"\n",
        "        self.test_outputs: Dict[str, List[torch.Tensor]] = {'layer1': [], 'layer2': [], 'layer3': []}\n",
        "        test_imgs: List[FloatNDArray] = []\n",
        "        gt_list: List[NDArray[Any]] = []\n",
        "        gt_mask_list: List[FloatNDArray] = []\n",
        "\n",
        "        # Extract test set features.\n",
        "        for x, y, mask in tqdm(val_dataloader, desc='Feature extraction (test)', disable=False):\n",
        "            x_shape = x.shape\n",
        "            test_imgs.extend(x.cpu().detach().numpy())\n",
        "            gt_list.extend(y.cpu().detach().numpy())\n",
        "            gt_mask_list.extend(mask.cpu().detach().numpy())\n",
        "            # Run model prediction.\n",
        "            with torch.no_grad():\n",
        "                _ = self.model(x.to(DEVICE))\n",
        "            # Get intermediate layer outputs.\n",
        "            assert list(self.outputs.keys())  == ['layer1', 'layer2', 'layer3']\n",
        "            for k, v in self.outputs.items():\n",
        "                self.test_outputs[k].append(v.cpu().detach())\n",
        "            # Reset hook outputs.\n",
        "            self.outputs = {}\n",
        "        gt_mask = np.asarray(gt_mask_list)  # shape (len(test_dataset), 1, H, W)\n",
        "\n",
        "        embedding_vectors = concatenate_embeddings_from_all_layers(\n",
        "            {k: torch.cat(v, 0) for k, v in self.test_outputs.items()})\n",
        "        # shape (len(test_dataset), len(feature_subset_indices), H1, W1)\n",
        "        embedding_vectors_subset = torch.index_select(embedding_vectors, 1, self.feature_subset_indices.cpu())\n",
        "\n",
        "        pxl_auroc_list = []\n",
        "        for feature in tqdm(list(features_to_permute), desc=\"Scoring feature importance\"):\n",
        "            permuted_embedding_vecs = self.permute_feature(embedding_vectors_subset, feature)\n",
        "\n",
        "            distances = self.calculate_distances(permuted_embedding_vecs)\n",
        "            score_map = self.prepare_anomaly_map((x_shape[2], x_shape[3]), distances)\n",
        "\n",
        "            pxl_fpr, pxl_tpr, pxl_auroc = self.calculate_auroc_pixel_level(score_map, gt_mask)\n",
        "            pxl_auroc_list.append(pxl_auroc)\n",
        "\n",
        "        return pxl_auroc_list\n",
        "\n",
        "    def permute_feature(self, embedding_vectors_subset: torch.Tensor, number_of_feature_to_permute: int) -> torch.Tensor:\n",
        "        \"\"\"Permutes the embeddings.\n",
        "\n",
        "        Takes embeddings of shape (N, C, H, W) and feature number to permute.\n",
        "        Returns embeddings with the same shape. See the description of T1 for the details.\n",
        "        \"\"\"\n",
        "        tensor = embedding_vectors_subset.clone()\n",
        "        b, c, h, w = tensor.shape\n",
        "        perm_h = torch.randperm(h).to(tensor.device)\n",
        "        perm_w = torch.randperm(w).to(tensor.device)\n",
        "\n",
        "        tensor[:, number_of_feature_to_permute, :, :] = tensor[:, number_of_feature_to_permute, perm_h, :]\n",
        "        tensor[:, number_of_feature_to_permute, :, :] = tensor[:, number_of_feature_to_permute, :, perm_w]\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    # TODO: End of your code for Task 1 (here)\n",
        "\n",
        "    def plot_test_results_for_class(self, gt_mask_list, test_imgs,\n",
        "                                    score_map, threshold: float,\n",
        "                                    img_fpr, img_tpr, img_auroc: float,\n",
        "                                    pxl_fpr, pxl_tpr, pxl_auroc: float):\n",
        "        _, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
        "        ax[0].plot(img_fpr, img_tpr, label=f'Image AUROC: {img_auroc:.3f}')\n",
        "        ax[1].plot(pxl_fpr, pxl_tpr, label=f'Pixel AUROC: {pxl_auroc:.3f}')\n",
        "\n",
        "        save_dir = self.save_path / f'pictures_{self.arch}'\n",
        "        save_dir.mkdir(parents=True, exist_ok=True)\n",
        "        plot_fig(test_imgs, score_map, gt_mask_list,\n",
        "                 threshold, save_dir, \"\")\n",
        "\n",
        "    def calculate_auroc_image_level(self, score_map: FloatNDArray, gt_list: List[NDArray[Any]]) -> Tuple[FloatNDArray, FloatNDArray, float]:\n",
        "        \"\"\"Calculate image-level AUROC score.\"\"\"\n",
        "        img_scores = score_map.reshape(score_map.shape[0], -1).max(axis=1)\n",
        "        fpr, tpr, _ = roc_curve(gt_list, img_scores)  # false-positive-rates and true-positive-rates for consecutive thresholds (for plotting).\n",
        "        img_auroc = roc_auc_score(gt_list, img_scores)\n",
        "        return fpr, tpr, float(img_auroc)\n",
        "\n",
        "    def calculate_auroc_pixel_level(self, score_map: FloatNDArray, gt_mask: FloatNDArray) -> Tuple[FloatNDArray, FloatNDArray, float]:\n",
        "        \"\"\"Calculate per-pixel level AUROC.\"\"\"\n",
        "        assert score_map.shape == gt_mask.squeeze().shape, f\"{score_map.shape=}, {gt_mask.shape=}\"\n",
        "        fpr, tpr, _ = roc_curve(gt_mask.flatten(), score_map.flatten())\n",
        "        per_pixel_auroc = roc_auc_score(gt_mask.flatten(), score_map.flatten())\n",
        "        return fpr, tpr, float(per_pixel_auroc)\n",
        "\n",
        "    def calculate_optimal_threshold(self, score_map: FloatNDArray, gt_mask: FloatNDArray) -> float:\n",
        "        \"\"\"Calculate the optimal threshold with regard to F1 score.\"\"\"\n",
        "        assert score_map.shape == gt_mask.squeeze().shape\n",
        "        precision, recall, thresholds = precision_recall_curve(\n",
        "            gt_mask.flatten(), score_map.flatten())\n",
        "        a = 2 * precision * recall\n",
        "        b = precision + recall\n",
        "        f1 = np.divide(a, b, out=np.zeros_like(a), where=(b != 0))\n",
        "        threshold = thresholds[np.argmax(f1)]\n",
        "        return threshold\n",
        "\n",
        "    def calculate_distances(self, embedding_vectors: torch.Tensor) -> FloatNDArray:\n",
        "        \"\"\"Calculate Mahalanobis distance of each embedding vector from self.mean.\n",
        "\n",
        "        For embeddings of shape (N, C, H, W), returns shape (N, H, W).\n",
        "        \"\"\"\n",
        "        B, C, H, W = embedding_vectors.size()\n",
        "        embedding_vectors = embedding_vectors.view(B, C, H * W).numpy()\n",
        "        dist_list: List[List[np.float64]] = []\n",
        "        for i in range(H * W):\n",
        "            mean = self.mean[:, i]\n",
        "            conv_inv = np.linalg.inv(self.cov[:, :, i])\n",
        "            dist = [mahalanobis(sample[:, i], mean, conv_inv)\n",
        "                    for sample in embedding_vectors]\n",
        "            dist_list.append(dist)\n",
        "\n",
        "        return np.array(dist_list).transpose(1, 0).reshape(B, H, W)\n",
        "\n",
        "    def prepare_anomaly_map(self, shape: Tuple[int, int], distances: FloatNDArray) -> FloatNDArray:\n",
        "        \"\"\"Upsample distances to `shape`, apply Gaussian smoothing, and normalize to [0,1].\n",
        "\n",
        "        For distances of shape (N, H, W) and `shape` equal to (H2, W2), returns shape (N, H2, W2).\n",
        "        \"\"\"\n",
        "        dists = torch.Tensor(distances).unsqueeze(1)\n",
        "        shape = (dists.shape[0],) + shape\n",
        "        score_map = cast(FloatNDArray, F.interpolate(\n",
        "            dists, size=shape[2], mode='bilinear', align_corners=False).squeeze().numpy())\n",
        "        for i in range(score_map.shape[0]):\n",
        "            score_map[i] = gaussian_filter(score_map[i], sigma=4)\n",
        "\n",
        "        min_score, max_score = score_map.min(), score_map.max()\n",
        "        return (score_map - min_score) / (max_score - min_score + 1e-10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BufmMzabVZxu"
      },
      "source": [
        "### Let's see whether it works.\n",
        "Take a look to the `SAVE_PATH` to inspect the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HlfOLsv3VZxv",
        "outputId": "b24cb2fe-87a7-43c3-a87b-df8355b3f44b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== bottle\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feature extraction (train): 100%|██████████| 105/105 [00:12<00:00,  8.71it/s]\n",
            "Covariance estimation: 100%|██████████| 3136/3136 [00:00<00:00, 4265.96it/s]\n",
            "Feature extraction (test): 100%|██████████| 42/42 [00:05<00:00,  8.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TEST] Image AUROC: 0.998\n",
            "[TEST] Pixel AUROC: 0.981\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "seed_all(SEED)\n",
        "CLASS_NAMES = [\n",
        "            'bottle', #'metal_nut'\n",
        "            # 'cable', 'capsule', 'carpet', 'grid', 'hazelnut', 'leather',\n",
        "            # 'pill', 'screw', 'tile', 'toothbrush', 'transistor', 'wood', 'zipper'\n",
        "        ]\n",
        "BATCH_SIZE = 2\n",
        "RESIZE = 256 * 1\n",
        "CROP_SIZE = 224 * 1\n",
        "BACKBONE = \"resnet18\"\n",
        "NUMBER_OF_BACKBONE_FEATURES = 50\n",
        "MAX_NUMBER_OF_BACKBONE_FEATURES = 448\n",
        "\n",
        "run_timestamp = time.time()\n",
        "for class_name in CLASS_NAMES:\n",
        "    print('=' * 10, class_name)\n",
        "    SAVE_PATH = Path(f\"./results/{run_timestamp}/{class_name}\")\n",
        "\n",
        "    train_dataset = MVTecDataset(DATA_PATH, class_name=class_name, is_train=True, resize=RESIZE, cropsize=CROP_SIZE)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
        "    test_dataset = MVTecDataset(DATA_PATH, class_name=class_name, is_train=False, resize=RESIZE, cropsize=CROP_SIZE)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
        "\n",
        "    padim = PADIM(\n",
        "        backbone=BACKBONE,\n",
        "        device=DEVICE,\n",
        "        backbone_features_idx=sample_idx(NUMBER_OF_BACKBONE_FEATURES, MAX_NUMBER_OF_BACKBONE_FEATURES),\n",
        "        save_path=SAVE_PATH,\n",
        "        plot_metrics=True,\n",
        "    )\n",
        "\n",
        "    padim.train_and_test(\n",
        "        train_dataloader=train_dataloader,\n",
        "        test_dataloader=test_dataloader,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL00CE8iVZxw"
      },
      "source": [
        "# Task 1. Finding the right features (40%)\n",
        "\n",
        "The authors of the paper argue that it doesn't really matter how we choose a subset of features. Let's make some steps towards exploring whether it's true for three different classes (`bottle`, `transistor`, `metal_nut`).\n",
        "Design an experiment which will rank the ResNet18 features by its importance. To do so, we'll implement our variation of [permutation feature importance](https://scikit-learn.org/stable/modules/permutation_importance.html#outline-of-the-permutation-importance-algorithm) on a subset of features produced by the backbone.\n",
        "\n",
        "## 1.1 Preparing the data\n",
        "- Using the test dataset, create `val_dataloader` (every even sample from the original test dataset) and `test_dataloader` (every odd sample). `SubsetRandomSampler` might be handy here.\n",
        "- Then, create 3-fold cross validation-like process in which you'll train three PADIM models on the first 100 ResNet features in three equally sized subsets of train dataset in which you discard 1/3 of the data ($\\texttt{padim}_{k}\\texttt{.train}(\\texttt{train\\_dataloader}_k)$) (see below). Again, `SubsetRandomSampler` might be handy here.\n",
        "\n",
        "In other words, you should have:\n",
        "\n",
        "- for $k=0$, the first 10 images indexes from the train dataset we should train on are `[1, 2, 4, 5, 7, 8, 10, 11, 13, 14]`,\n",
        "- for $k=1$, that's `[0, 2, 3, 5, 6, 8, 9, 11, 12, 14]`,\n",
        "- and for $k=2$, that's `[0, 1, 3, 4, 6, 7, 9, 10, 12, 13]`.\n",
        "\n",
        "For val and train, you should have `[0, 2, 4, ...]` and `[1, 3, 5, ...]` respectively (from the test dataset).\n",
        "\n",
        "Don't worry about the sampling order.\n",
        "Use these names for DataLoaders `val_dataloader`, `test_dataloader`. For k-fold training, store dataloaders in `train_dataloaders: List[DataLoader]`, where each element represent different $k$.\n",
        "For each class, store the results in `dataloaders` dictionary (the variable is defined in the code below) - we will use this to check your solution.\n",
        "\n",
        "## 1.2 Calculating the importances\n",
        "- In a given fold, each $j$-th feature shall be ranked based on the pixel-wise AUROC difference between the output of that model ($s_{k} \\leftarrow \\texttt{padim}_{k}\\texttt{.test}(\\texttt{val\\_dataloader})$) and the output with the model with permuted $j$-th feature ($s_{k, j} \\leftarrow \\texttt{padim}_{k}\\texttt{.test\\_permutation\\_importance}(\\texttt{val\\_dataloader, features\\_to\\_permute=}[j])$). In practice you can pass all the numbers of features to permute (instead of 1-element list and do the loop inside the method. See also `test_permutation_importance` method stub above.\n",
        "- Implement `permute_feature` method as follows: given the tensor with embeddings with shape `[B, C, H, W]`, by permutation of the $j$-th feature we mean randomly swapped values for $C=j$. Although (ideally) the order of swapping shall be **different** for every image, we don't require you to strictly guarantee that you won't get the same permutation twice (what matters here is not using the same permutation for **every** sample - you can e.g. use distinct calls to a shuffling function for every sample). In other words, for every image $b$ and feature $j$ you need to shuffle the last two dimensions (marked as stars in `[b, j, *, *]`) in an (ideally) unique manner.\n",
        "- Then, calculate the mean importance $i$ averaged on these folds and plot weights importance for the class ($i_j \\leftarrow \\frac{1}{K} \\sum_{k} ( s_k -  s_{k, j} )$, where $K$ is the number of folds).\n",
        "- Append results in `results` dictionary, where keys are class names and values are the lists of averaged feature importances (from feature 0 to feature 99).\n",
        "\n",
        "## 1.3 Drawing conclusions\n",
        "\n",
        "- Finally, for every class train three models on the full training data and evaluate it on the `test_dataloader`. The first model shall use the first 10 features, the second shall use worst 10 features (in terms of feature importance), and the third shall contain the best 10 features.\n",
        "- Write your conclusions (with the things enlisted below in mind). Simply plotting charts or outputting logs without any comment doesn't qualify as an answer to a question.\n",
        "\n",
        "Note 1: Limit yourself to the first 100 features of ResNet18. If you want, you can go with all of available features instead of 100, but it'll take some time to calculate. Converting parts of the code to PyTorch and running on GPU might change a lot here, but this is not evaluated in this exercise. This experiment can be calculated without GPU in less than one hour anyway.\n",
        "\n",
        "Note 2: If you'd like to be fully covered, one needs to explore if the features are correlated, as this might bias the results of feature importance calculations. However, this is not evaluated in this task for the sake of simplicity (that is, examining the 100 first features without worrying about correlated features are enough to get 100% from this task)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LzYc9ExXVZxw"
      },
      "outputs": [],
      "source": [
        "# do not modify\n",
        "CLASS_NAMES = ['bottle'] # ['bottle', 'metal_nut', 'transistor'] \n",
        "\n",
        "BATCH_SIZE = 1\n",
        "RESIZE = 256 * 2 // 4\n",
        "CROP_SIZE = 224 * 2 // 4\n",
        "BACKBONE = \"resnet18\"\n",
        "NUMBER_OF_BACKBONE_FEATURES = 10\n",
        "MAX_NUMBER_OF_BACKBONE_FEATURES = 100  # 448\n",
        "folds = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3UHb3HymVZxx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1702918790.057964\n"
          ]
        }
      ],
      "source": [
        "seed_all(SEED)\n",
        "results = {c: [0] * MAX_NUMBER_OF_BACKBONE_FEATURES for c in CLASS_NAMES}\n",
        "\n",
        "run_timestamp = time.time()\n",
        "print(f\"{run_timestamp}\")\n",
        "\n",
        "idx_all_fatures = torch.Tensor(range(MAX_NUMBER_OF_BACKBONE_FEATURES)).int()\n",
        "idx_first_n_features = torch.Tensor(range(NUMBER_OF_BACKBONE_FEATURES)).int()\n",
        "\n",
        "dataloaders = {c: {\"val_dataloader\": None, \"test_dataloader\": None, \"train_dataloaders\": None} for c in CLASS_NAMES}\n",
        "\n",
        "# TODO: Your code for T1.1, T1.2, and T1.3 goes below. Don't forget to write `test_permutation_importance` and `permute_feature` above in the PADIM code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# T1.1\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "\n",
        "\n",
        "for class_name in CLASS_NAMES:\n",
        "    test_dataset = MVTecDataset(DATA_PATH, class_name=class_name, is_train=False, resize=RESIZE, cropsize=CROP_SIZE)\n",
        "    train_dataset = MVTecDataset(DATA_PATH, class_name=class_name, is_train=True, resize=RESIZE, cropsize=CROP_SIZE)\n",
        "    \n",
        "    odd_indices = list(range(len(test_dataset)))[::2]\n",
        "    even_indices = list(range(len(test_dataset)))[1::2]\n",
        "\n",
        "    val_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True, \n",
        "                                sampler=SubsetRandomSampler(even_indices))\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True,\n",
        "                                sampler=SubsetRandomSampler(odd_indices))\n",
        "    \n",
        "    train_dataloaders = []\n",
        "    for fold in range(folds):\n",
        "        train_indices = [idx for idx in list(range(len(train_dataset))) if idx % folds != fold]\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True,\n",
        "                                    sampler=SubsetRandomSampler(train_indices))\n",
        "        train_dataloaders.append(train_dataloader)\n",
        "\n",
        "    dataloaders[class_name] = {\"val_dataloader\": val_dataloader, \"test_dataloader\": test_dataloader, \"train_dataloaders\": train_dataloaders}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== bottle\n",
            "Fold: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feature extraction (train): 100%|██████████| 139/139 [00:08<00:00, 17.25it/s]\n",
            "Covariance estimation: 100%|██████████| 784/784 [00:00<00:00, 2965.21it/s]\n",
            "Feature extraction (test): 100%|██████████| 41/41 [00:02<00:00, 16.20it/s]\n",
            "Feature extraction (test): 100%|██████████| 41/41 [00:02<00:00, 16.59it/s]\n",
            "Scoring feature importance:   3%|▎         | 3/100 [00:16<08:58,  5.55s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m features_to_permute \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(MAX_NUMBER_OF_BACKBONE_FEATURES))\n\u001b[1;32m     20\u001b[0m score_k \u001b[38;5;241m=\u001b[39m padim_k\u001b[38;5;241m.\u001b[39mtest(val_dataloader)\n\u001b[0;32m---> 21\u001b[0m score_k_j_list \u001b[38;5;241m=\u001b[39m \u001b[43mpadim_k\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_permutation_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_to_permute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_NUMBER_OF_BACKBONE_FEATURES):\n\u001b[1;32m     24\u001b[0m     results[class_name][j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (score_k \u001b[38;5;241m-\u001b[39m score_k_j_list[j]) \u001b[38;5;241m/\u001b[39m folds\n",
            "Cell \u001b[0;32mIn[9], line 156\u001b[0m, in \u001b[0;36mPADIM.test_permutation_importance\u001b[0;34m(self, val_dataloader, features_to_permute)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mlist\u001b[39m(features_to_permute), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScoring feature importance\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    154\u001b[0m     permuted_embedding_vecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_feature(embedding_vectors_subset, feature)\n\u001b[0;32m--> 156\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpermuted_embedding_vecs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     score_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_anomaly_map((x_shape[\u001b[38;5;241m2\u001b[39m], x_shape[\u001b[38;5;241m3\u001b[39m]), distances)\n\u001b[1;32m    159\u001b[0m     pxl_fpr, pxl_tpr, pxl_auroc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_auroc_pixel_level(score_map, gt_mask)\n",
            "Cell \u001b[0;32mIn[9], line 231\u001b[0m, in \u001b[0;36mPADIM.calculate_distances\u001b[0;34m(self, embedding_vectors)\u001b[0m\n\u001b[1;32m    229\u001b[0m     mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean[:, i]\n\u001b[1;32m    230\u001b[0m     conv_inv \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcov[:, :, i])\n\u001b[0;32m--> 231\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mmahalanobis\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_inv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43membedding_vectors\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    233\u001b[0m     dist_list\u001b[38;5;241m.\u001b[39mappend(dist)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(dist_list)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, H, W)\n",
            "Cell \u001b[0;32mIn[9], line 231\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    229\u001b[0m     mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean[:, i]\n\u001b[1;32m    230\u001b[0m     conv_inv \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcov[:, :, i])\n\u001b[0;32m--> 231\u001b[0m     dist \u001b[38;5;241m=\u001b[39m [\u001b[43mmahalanobis\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_inv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m embedding_vectors]\n\u001b[1;32m    233\u001b[0m     dist_list\u001b[38;5;241m.\u001b[39mappend(dist)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(dist_list)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, H, W)\n",
            "File \u001b[0;32m~/work/dnn/.venv/lib/python3.11/site-packages/scipy/spatial/distance.py:1022\u001b[0m, in \u001b[0;36mmahalanobis\u001b[0;34m(u, v, VI)\u001b[0m\n\u001b[1;32m   1020\u001b[0m VI \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_2d(VI)\n\u001b[1;32m   1021\u001b[0m delta \u001b[38;5;241m=\u001b[39m u \u001b[38;5;241m-\u001b[39m v\n\u001b[0;32m-> 1022\u001b[0m m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVI\u001b[49m\u001b[43m)\u001b[49m, delta)\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqrt(m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# T1.2\n",
        "\n",
        "for class_name in CLASS_NAMES:\n",
        "    print('=' * 10, class_name)\n",
        "    val_dataloader = dataloaders[class_name][\"val_dataloader\"]\n",
        "    test_dataloader = dataloaders[class_name][\"test_dataloader\"]\n",
        "    train_dataloaders = dataloaders[class_name][\"train_dataloaders\"]\n",
        "\n",
        "    for fold in range(folds):\n",
        "        print(f\"Fold: {fold}\")\n",
        "        padim_k = PADIM(\n",
        "            backbone=BACKBONE,\n",
        "            device=DEVICE,\n",
        "            backbone_features_idx=idx_all_fatures,\n",
        "            save_path=SAVE_PATH,\n",
        "            plot_metrics=False,\n",
        "        )\n",
        "        padim_k.train(train_dataloaders[fold])\n",
        "        features_to_permute = list(range(MAX_NUMBER_OF_BACKBONE_FEATURES))\n",
        "        score_k = padim_k.test(val_dataloader)\n",
        "        score_k_j_list = padim_k.test_permutation_importance(val_dataloader, features_to_permute)\n",
        "        \n",
        "        for j in range(MAX_NUMBER_OF_BACKBONE_FEATURES):\n",
        "            results[class_name][j] += (score_k - score_k_j_list[j]) / folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== bottle\n",
            "========== first_10_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feature extraction (train): 100%|██████████| 209/209 [00:11<00:00, 17.61it/s]\n",
            "Covariance estimation: 100%|██████████| 784/784 [00:00<00:00, 24068.14it/s]\n",
            "Feature extraction (test): 100%|██████████| 42/42 [00:02<00:00, 15.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TEST] Image AUROC: 1.000\n",
            "[TEST] Pixel AUROC: 0.975\n",
            "========== most_important_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feature extraction (train):  68%|██████▊   | 143/209 [00:08<00:03, 17.76it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 27\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m, feature_set_name)\n\u001b[1;32m     19\u001b[0m padim \u001b[38;5;241m=\u001b[39m PADIM(\n\u001b[1;32m     20\u001b[0m     backbone\u001b[38;5;241m=\u001b[39mBACKBONE,\n\u001b[1;32m     21\u001b[0m     device\u001b[38;5;241m=\u001b[39mDEVICE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     plot_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 27\u001b[0m \u001b[43mpadim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_dataloader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[9], line 40\u001b[0m, in \u001b[0;36mPADIM.train_and_test\u001b[0;34m(self, train_dataloader, test_dataloader)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_and_test\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_dataloader: DataLoader, test_dataloader: DataLoader) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest(test_dataloader)\n",
            "Cell \u001b[0;32mIn[9], line 45\u001b[0m, in \u001b[0;36mPADIM.train\u001b[0;34m(self, train_dataloader)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_dataloader: DataLoader) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, List[torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer1\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer2\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer3\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n\u001b[0;32m---> 45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFeature extraction (train)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Run model prediction.\u001b[39;49;00m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/work/dnn/.venv/lib/python3.11/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1185\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
            "File \u001b[0;32m~/work/dnn/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/work/dnn/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/work/dnn/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/work/dnn/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[0;32mIn[6], line 43\u001b[0m, in \u001b[0;36mMVTecDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m idx\n\u001b[1;32m     41\u001b[0m x, y, mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx[idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask[idx]\n\u001b[0;32m---> 43\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m x \u001b[38;5;241m=\u001b[39m cast(torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_x(x))\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m~/work/dnn/.venv/lib/python3.11/site-packages/PIL/Image.py:916\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[1;32m    870\u001b[0m ):\n\u001b[1;32m    871\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 916\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    918\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
            "File \u001b[0;32m~/work/dnn/.venv/lib/python3.11/site-packages/PIL/ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# T1.3\n",
        "\n",
        "for class_name in CLASS_NAMES:\n",
        "    print('=' * 10, class_name)\n",
        "    train_dataset = MVTecDataset(DATA_PATH, class_name=class_name, is_train=True, resize=RESIZE, cropsize=CROP_SIZE)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
        "\n",
        "    feature_importance = results[class_name]\n",
        "    sorted_f_imp_idx = sorted(enumerate(feature_importance), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    feature_sets = {\n",
        "        \"first_10_features\" : list(range(10)),\n",
        "        \"most_important_features\" : [index for index, number in sorted_f_imp_idx[:10]],\n",
        "        \"least_important_features\" : [index for index, number in sorted_f_imp_idx[-10:]],\n",
        "    } \n",
        "\n",
        "    for feature_set_name, feature_set in feature_sets.items():\n",
        "        print(\"=\" * 10, feature_set_name)\n",
        "        padim = PADIM(\n",
        "            backbone=BACKBONE,\n",
        "            device=DEVICE,\n",
        "            backbone_features_idx=torch.Tensor(feature_set).int(),\n",
        "            save_path=SAVE_PATH,\n",
        "            plot_metrics=True,\n",
        "        )\n",
        "\n",
        "        padim.train_and_test(train_dataloader, dataloaders[class_name][\"test_dataloader\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T1.3.: Thoughts and conclusion\n",
        "\n",
        "Our results indicate that specific choice of features doesn't seem to matter in the grand picture. My hypothesis is that resNet tries to encode the most important data through the consecutive layers and thus no matter from which layer we would choose our features the information relevant for classification is still encoded in them. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "za2w3WO3VZxx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 63, 73, 72, 71, 70, 69, 68, 67, 66]\n",
            "[0, 72, 71, 70, 69, 68, 67, 66, 65, 64]\n",
            "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81]\n",
            "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82]\n",
            "[1, 2, 4, 5, 7, 8, 10, 11, 13, 14, 16, 17, 19, 20, 22, 23, 25, 26, 28, 29, 31, 32, 34, 35, 37, 38, 40, 41, 43, 44, 46, 47, 49, 50, 52, 53, 55, 56, 58, 59, 61, 62, 64, 65, 67, 68, 70, 71, 73, 74, 76, 77, 79, 80, 82, 83, 85, 86, 88, 89, 91, 92, 94, 95, 97, 98, 100, 101, 103, 104, 106, 107, 109, 110, 112, 113, 115, 116, 118, 119, 121, 122, 124, 125, 127, 128, 130, 131, 133, 134, 136, 137, 139, 140, 142, 143, 145, 146, 148, 149, 151, 152, 154, 155, 157, 158, 160, 161, 163, 164, 166, 167, 169, 170, 172, 173, 175, 176, 178, 179, 181, 182, 184, 185, 187, 188, 190, 191, 193, 194, 196, 197, 199, 200, 202, 203, 205, 206, 208]\n",
            "[0, 2, 3, 5, 6, 8, 9, 11, 12, 14, 15, 17, 18, 20, 21, 23, 24, 26, 27, 29, 30, 32, 33, 35, 36, 38, 39, 41, 42, 44, 45, 47, 48, 50, 51, 53, 54, 56, 57, 59, 60, 62, 63, 65, 66, 68, 69, 71, 72, 74, 75, 77, 78, 80, 81, 83, 84, 86, 87, 89, 90, 92, 93, 95, 96, 98, 99, 101, 102, 104, 105, 107, 108, 110, 111, 113, 114, 116, 117, 119, 120, 122, 123, 125, 126, 128, 129, 131, 132, 134, 135, 137, 138, 140, 141, 143, 144, 146, 147, 149, 150, 152, 153, 155, 156, 158, 159, 161, 162, 164, 165, 167, 168, 170, 171, 173, 174, 176, 177, 179, 180, 182, 183, 185, 186, 188, 189, 191, 192, 194, 195, 197, 198, 200, 201, 203, 204, 206, 207]\n",
            "[0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 15, 16, 18, 19, 21, 22, 24, 25, 27, 28, 30, 31, 33, 34, 36, 37, 39, 40, 42, 43, 45, 46, 48, 49, 51, 52, 54, 55, 57, 58, 60, 61, 63, 64, 66, 67, 69, 70, 72, 73, 75, 76, 78, 79, 81, 82, 84, 85, 87, 88, 90, 91, 93, 94, 96, 97, 99, 100, 102, 103, 105, 106, 108, 109, 111, 112, 114, 115, 117, 118, 120, 121, 123, 124, 126, 127, 129, 130, 132, 133, 135, 136, 138, 139, 141, 142, 144, 145, 147, 148, 150, 151, 153, 154, 156, 157, 159, 160, 162, 163, 165, 166, 168, 169, 171, 172, 174, 175, 177, 178, 180, 181, 183, 184, 186, 187, 189, 190, 192, 193, 195, 196, 198, 199, 201, 202, 204, 205, 207, 208]\n"
          ]
        }
      ],
      "source": [
        "# Run at the end, but do not modify - we will use this to asses your output.\n",
        "for c in CLASS_NAMES:\n",
        "    s = pd.Series(results[c])\n",
        "    print(s.sort_values(ascending=False)[:10].index.tolist())\n",
        "    print(s.sort_values(ascending=True)[:10].index.tolist())\n",
        "\n",
        "def get_sorted_indices(loader):\n",
        "    loader.dataset.return_only_indices = True\n",
        "    indices = sorted([x.item() for x in loader])\n",
        "    loader.dataset.return_only_indices = False\n",
        "    return indices\n",
        "\n",
        "for c in CLASS_NAMES:\n",
        "    print(get_sorted_indices(dataloaders[c][\"val_dataloader\"]))\n",
        "    print(get_sorted_indices(dataloaders[c][\"test_dataloader\"]))\n",
        "    for v in dataloaders[c][\"train_dataloaders\"]:\n",
        "        print(get_sorted_indices(v))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qtn9ylbVZxx"
      },
      "source": [
        "# Task 2. Improving PADIM with Online Covariance Estimation\n",
        "\n",
        "This implementation of PADIM can be improved in numerous ways. In this exercise, you'll try to indicate its shortcomings and provide some means to mitigate them.\n",
        "\n",
        "#### 2.1. PADIM's training complexity (15%)\n",
        "\n",
        "- Identify the key operations contributing to the algorithm's training space complexity *in this implementation*. Don't focus on the backbone, as it is not the part of the algorithm (however, its output is).\n",
        "- Shortly discuss the implications for scalability. You can support your claims by charts if needed.\n",
        "\n",
        "*Hint: this doesn't need to be super formal analysis - it's about fiding the \"worst\" parts of this implementation. You can support your claims with a chart and brief description (e.g. \"X dominates the complexity, as it's quadratic.\")*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdODnlnsVZxx"
      },
      "source": [
        "```Your answer to task 2.1 goes here```\n",
        "\n",
        "Dominating part of the training procedure is the estimation of mulitivariate gaussian since it is quadratic with respect to the number of features. One could argue that the complexity of this estimation is even greater as each feature is represented by a matrix. But in our case maximum feature matrix size is constant and equal to 52 x 52 so can be disregarded. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fD9esI1fVZxx"
      },
      "outputs": [],
      "source": [
        "# Your code goes here (if needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by7h9sp3VZxx"
      },
      "source": [
        "#### 2.2 Online mean and covariance (35%)\n",
        "Implement a PyTorch version of [online covariance matrix estimation](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online) in the training as an alternative to the current method in PADIM.\n",
        "Calculate the mean in an online fashion as well.\n",
        "Your implementation shall run on the selected `torch.device` (such as GPU).\n",
        "No need to reimplement the testing routine to online in this exercise (although it'd be nice to have for Task 1), albeit small changes might be necessary (such as conversion from `torch.Tensor` to `np.ndarray`).\n",
        "\n",
        "Passing criteria:\n",
        "```python\n",
        "torch.allclose(padim_online.mean, torch.Tensor(padim_offline.mean).to(DEVICE), atol=0.01)\n",
        "torch.allclose(padim_online.cov, torch.Tensor(padim_offline.cov).to(DEVICE), atol=0.01)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "E-1wMyouVZxx"
      },
      "outputs": [],
      "source": [
        "class PADIMWithOnlineCovariance(PADIM):\n",
        "\n",
        "    ### TODO: Your code goes here\n",
        "    def __init__(\n",
        "            self,\n",
        "            backbone: str,\n",
        "            device: torch.device,\n",
        "            save_path: Path,\n",
        "            backbone_features_idx: List[int],\n",
        "            class_names=...,\n",
        "            plot_metrics=False,\n",
        "            ) -> None:\n",
        "        super().__init__(backbone, device, save_path, backbone_features_idx, class_names, plot_metrics)\n",
        "\n",
        "    \n",
        "\n",
        "    def train(self, train_dataloader: DataLoader, C: int, H: int, W: int):\n",
        "        \"\"\"C, H, W come from the size of embeddings: [B, C, H, W]\"\"\"\n",
        "        self.train_outputs: Dict[str, List[torch.Tensor]] = {'layer1': [], 'layer2': [], 'layer3': []}\n",
        "        for x, _, _ in tqdm(train_dataloader, desc='Feature extraction (train)'):\n",
        "            # Run model prediction.\n",
        "            with torch.no_grad():\n",
        "                _ = self.model(x.to(DEVICE))\n",
        "            # Get intermediate layer outputs.\n",
        "            assert list(self.outputs.keys())  == ['layer1', 'layer2', 'layer3'], list(self.outputs.keys())\n",
        "            for k, v in self.outputs.items():\n",
        "                self.train_outputs[k].append(v)\n",
        "            # Reset hook outputs.\n",
        "            self.outputs = {}\n",
        "        \n",
        "\n",
        "        embedding_vectors = concatenate_embeddings_from_all_layers(\n",
        "            {k: torch.cat(v, 0) for k, v in self.train_outputs.items()})\n",
        "        embedding_vectors_subset = torch.index_select(embedding_vectors, 1, self.feature_subset_indices)\n",
        "\n",
        "        self.mean, self.cov = self.estimate_multivariate_gaussian_online(embedding_vectors_subset)\n",
        "        del(self.train_outputs)\n",
        "\n",
        "\n",
        "    def estimate_multivariate_gaussian_online(self, embedding_vectors: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Calculate the online covariance for a 4D tensor of shape (B, C, H, W).\n",
        "        Treats each (C, H, W) tensor as a data point, and calculates the covariance \n",
        "        across the batch dimension using optimized matrix operations.\n",
        "\n",
        "        :param embedding_vectors: A 4D torch tensor with shape (B, C, H, W).\n",
        "        :return: A tuple containing the mean of shape (C, H*W) and the covariance \n",
        "                matrix of shape (C, C, H*W).\n",
        "        \"\"\"\n",
        "        device = embedding_vectors.device  # Get the device of the input tensor\n",
        "        B, C, H, W = embedding_vectors.size()\n",
        "        embedding_vectors = embedding_vectors.view(B, C, H * W)  # images, features, values\n",
        "\n",
        "        mean = torch.zeros(C, H * W).to(device)\n",
        "        cov = torch.zeros(C, C, H * W).to(device)\n",
        "        n = 0\n",
        "        for i in range(B):\n",
        "            n += 1\n",
        "            sample = embedding_vectors[i]  # features, values\n",
        "            delta = sample - mean\n",
        "            mean += delta / n\n",
        "\n",
        "            # Optimized matrix operation for covariance update\n",
        "            delta_outer = delta.unsqueeze(1) * (sample - mean).unsqueeze(0)\n",
        "            cov += delta_outer\n",
        "\n",
        "        cov /= (n - 1)\n",
        "\n",
        "        return mean, cov\n",
        "\n",
        "    ### END OF YOUR CODE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "7TT0JEi5VZxx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feature extraction (train): 100%|██████████| 209/209 [00:12<00:00, 17.13it/s]\n",
            "Covariance estimation: 100%|██████████| 3136/3136 [00:00<00:00, 11667.17it/s]\n",
            "Feature extraction (train): 100%|██████████| 209/209 [00:11<00:00, 18.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mps:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# do not modify\n",
        "seed_all(SEED)\n",
        "class_name = 'bottle'\n",
        "BATCH_SIZE = 1\n",
        "RESIZE = 256 * 1\n",
        "CROP_SIZE = 224 * 1\n",
        "BACKBONE = \"resnet18\"\n",
        "NUMBER_OF_BACKBONE_FEATURES = 30\n",
        "MAX_NUMBER_OF_BACKBONE_FEATURES = 448\n",
        "# DEVICE=\"cpu\"\n",
        "\n",
        "indices = sample_idx(NUMBER_OF_BACKBONE_FEATURES, MAX_NUMBER_OF_BACKBONE_FEATURES).to(DEVICE)\n",
        "\n",
        "run_timestamp = time.time()\n",
        "SAVE_PATH = Path(f\"./results/{run_timestamp}/{class_name}\")\n",
        "\n",
        "train_dataset = MVTecDataset(DATA_PATH, class_name=class_name, is_train=True, resize=RESIZE, cropsize=CROP_SIZE)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
        "test_dataset = MVTecDataset(DATA_PATH, class_name=class_name, is_train=False, resize=RESIZE, cropsize=CROP_SIZE)\n",
        "val_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
        "\n",
        "\n",
        "padim_offline = PADIM(\n",
        "    backbone=BACKBONE,\n",
        "    device=DEVICE,\n",
        "    backbone_features_idx=indices,\n",
        "    save_path=SAVE_PATH,\n",
        "    plot_metrics=True,\n",
        ")\n",
        "padim_offline.train(train_dataloader)\n",
        "\n",
        "padim_online = PADIMWithOnlineCovariance(\n",
        "    backbone=BACKBONE,\n",
        "    device=DEVICE,\n",
        "    backbone_features_idx=indices,\n",
        "    save_path=SAVE_PATH,\n",
        "    plot_metrics=True,\n",
        ")\n",
        "padim_online.train(train_dataloader, NUMBER_OF_BACKBONE_FEATURES, int(CROP_SIZE/4), int(CROP_SIZE/4))\n",
        "\n",
        "torch.allclose(padim_online.mean, torch.Tensor(padim_offline.mean).to(DEVICE), atol=0.01) and torch.allclose(padim_online.cov, torch.Tensor(padim_offline.cov).to(DEVICE), atol=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQEY2gVfVZxx"
      },
      "source": [
        "#### 2.3 Performance experiments (10%)\n",
        "If you completed task 2.2, design experiments to empirically compare `space/memory` performance of PADIM training with both traditional and online covariance matrix estimation. Write short conclusions.\n",
        "\n",
        "#### 2.4 Bonus task (optional)\n",
        "You can also add similar experiments with conclusions with regard to the `time` complexity. This task is optional, but if you'll loose points elsewhere, this can help you to make up for some of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "STAGE:2023-12-18 19:13:57 24802:12246133 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "Feature extraction (train): 100%|██████████| 209/209 [00:14<00:00, 14.14it/s]\n",
            "Covariance estimation: 100%|██████████| 3136/3136 [00:00<00:00, 10690.77it/s]\n",
            "STAGE:2023-12-18 19:14:15 24802:12246133 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2023-12-18 19:14:15 24802:12246133 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== CPU Memory Usage\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::empty         0.05%       7.670ms         0.05%       7.670ms       0.223us      12.68 Gb      12.68 Gb         34430  \n",
            "                                              aten::cat         1.69%     283.869ms         1.70%     286.237ms     649.063us       1.99 Gb       1.99 Gb           441  \n",
            "                                          aten::resize_         0.02%       2.581ms         0.02%       2.581ms       0.617us       1.18 Gb       1.18 Gb          4184  \n",
            "                                       aten::empty_like         0.03%       4.407ms         0.04%       5.917ms       1.347us       3.94 Gb       1.06 Gb          4393  \n",
            "                          aten::max_pool2d_with_indices         1.37%     229.922ms         1.37%     229.922ms       1.100ms     480.05 Mb     480.05 Mb           209  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 16.813s\n",
            "\n",
            "========== CUDA Memory Usage\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::empty         0.05%       7.670ms         0.05%       7.670ms       0.223us      12.68 Gb      12.68 Gb         34430  \n",
            "                                          aten::random_         0.00%       8.000us         0.00%       8.000us       8.000us           0 b           0 b             1  \n",
            "                                             aten::item         0.00%     406.000us         0.00%     531.000us       2.529us          -2 b          -2 b           210  \n",
            "                              aten::_local_scalar_dense         0.00%     179.000us         0.00%     179.000us       0.852us          -3 b          -3 b           210  \n",
            "                                               [memory]         0.00%       0.000us         0.00%       0.000us       0.000us      -9.86 Gb      -9.86 Gb          9657  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 16.813s\n",
            "\n"
          ]
        }
      ],
      "source": [
        "padim_offline = PADIM(\n",
        "    backbone=BACKBONE,\n",
        "    device=DEVICE,\n",
        "    backbone_features_idx=indices,\n",
        "    save_path=SAVE_PATH,\n",
        "    plot_metrics=False,\n",
        ")\n",
        "\n",
        "with torch.profiler.profile(\n",
        "        activities=[torch.profiler.ProfilerActivity.CPU,\n",
        "                    torch.profiler.ProfilerActivity.CUDA],  \n",
        "        profile_memory=True, \n",
        "        record_shapes=True) as prof:  \n",
        "    padim_offline.train(train_dataloader)\n",
        "\n",
        "print(\"=\" * 10, \"CPU Memory Usage\")\n",
        "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "STAGE:2023-12-18 19:15:16 24802:12246133 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "Feature extraction (train): 100%|██████████| 209/209 [00:15<00:00, 13.68it/s]\n",
            "Covariance estimation: 100%|██████████| 3136/3136 [00:00<00:00, 7457.20it/s]\n",
            "STAGE:2023-12-18 19:15:35 24802:12246133 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2023-12-18 19:15:35 24802:12246133 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== CPU Memory Usage\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::empty         0.06%      10.188ms         0.06%      10.188ms       0.296us      13.31 Gb      13.31 Gb         34446  \n",
            "                                              aten::cat         1.68%     296.652ms         1.70%     299.294ms     678.671us       1.99 Gb       1.99 Gb           441  \n",
            "                                          aten::resize_         0.02%       3.627ms         0.02%       3.627ms       0.867us       1.22 Gb       1.22 Gb          4184  \n",
            "                                       aten::empty_like         0.03%       4.696ms         0.04%       6.616ms       1.506us       3.97 Gb     928.27 Mb          4393  \n",
            "                          aten::max_pool2d_with_indices         1.43%     252.364ms         1.43%     252.364ms       1.207ms     476.98 Mb     476.98 Mb           209  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 17.656s\n",
            "\n"
          ]
        }
      ],
      "source": [
        "padim_online = PADIMWithOnlineCovariance(\n",
        "    backbone=BACKBONE,\n",
        "    device=DEVICE,\n",
        "    backbone_features_idx=indices,\n",
        "    save_path=SAVE_PATH,\n",
        "    plot_metrics=False,\n",
        ")\n",
        "\n",
        "with torch.profiler.profile(\n",
        "        activities=[torch.profiler.ProfilerActivity.CPU,\n",
        "                    torch.profiler.ProfilerActivity.CUDA],  \n",
        "        profile_memory=True, \n",
        "        record_shapes=True) as prof: \n",
        "    padim_offline.train(train_dataloader)\n",
        "\n",
        "print(\"=\" * 10, \"CPU Memory Usage\")\n",
        "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKuh7U-oVZxy"
      },
      "source": [
        "```Your conclusions go here```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml-teaching",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
