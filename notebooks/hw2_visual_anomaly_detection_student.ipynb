{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISUBZbOTVZxn"
      },
      "source": [
        "# Low-shot visual anomaly detection (v2.1)\n",
        "\n",
        "*This is the updated version of the homework with some additional clarifications and changes in the task descriptions (see slack channel for details). The code remains untouched.*\n",
        "\n",
        "In this notebook you'll investigate visual anomaly detection in a typical industrial setting - we don't have much data and we can train only only normal (non-anomalous) examples.\n",
        "Read the [PADIM paper](https://arxiv.org/pdf/2011.08785.pdf) carefully.\n",
        "The code here is based on the original implementation from its authors.\n",
        "\n",
        "If you have any questions - please write them on slack in the channel.\n",
        "\n",
        "### Bibliography\n",
        "\n",
        "1. Defard, T., Setkov, A., Loesch, A., & Audigier, R. (2021). [Padim: a patch distribution modeling framework for anomaly detection and localization](https://arxiv.org/pdf/2011.08785.pdf). In International Conference on Pattern Recognition (pp. 475-489). Cham: Springer International Publishing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Gwf82SgVZxo"
      },
      "source": [
        "## Data\n",
        "\n",
        "In case of any problems - please visit [MVTec AD](https://www.mvtec.com/company/research/datasets/mvtec-ad/downloads) to get the access to the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yRQ3gPIVZxp"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet gdown  # for those who don't run it on Google Colab\n",
        "!gdown -q '1r7WJeDb-E5zzgQSOx7F7bNWg8kYX3yKE'\n",
        "!gdown -q '1Kb420ygkN1iBni5Iy_-psLGNoY0gQFk9'\n",
        "!gdown -q '12wDP9I3aVIr1qLekWY3GLhQO7c6SRhGn'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "liTm9VH-VZxp"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import tarfile\n",
        "\n",
        "DATA_PATH = Path('./mvtec_anomaly_detection')\n",
        "DATA_PATH.mkdir(exist_ok=True)\n",
        "\n",
        "for class_name in ['bottle', 'metal_nut', 'transistor']:\n",
        "    if not (DATA_PATH / class_name).exists():\n",
        "        with tarfile.open(class_name + '.tar.xz') as tar:\n",
        "            tar.extractall(path=DATA_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQyQYffqVZxq"
      },
      "source": [
        "## PADIM implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uvPuyg5vVZxq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "from random import sample\n",
        "from typing import cast, Any, Dict, List, Optional, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.backends, torch.backends.mps\n",
        "import torch.nn.functional as F\n",
        "from numpy.typing import NDArray\n",
        "from matplotlib import colors\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from skimage import morphology\n",
        "from skimage.segmentation import mark_boundaries\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models import wide_resnet50_2, resnet18, Wide_ResNet50_2_Weights, ResNet18_Weights\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "FloatNDArray = NDArray[np.float32]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SuSoeQJMVZxr"
      },
      "outputs": [],
      "source": [
        "# Leave it as is if you're unsure, this notebook will guess this for you below.\n",
        "DEVICE: Optional[torch.device] = None\n",
        "SEED: int = 42  # do not modify\n",
        "\n",
        "plt.style.use(\"dark_background\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "w6q4z2STVZxr"
      },
      "outputs": [],
      "source": [
        "def seed_all(seed: int = 0) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def get_best_device_for_pytorch() -> torch.device:\n",
        "    if torch.cuda.is_available():\n",
        "        device_str = \"cuda\"     # GPU\n",
        "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        device_str = \"mps\"      # Apple silicon\n",
        "    else:\n",
        "        device_str = \"cpu\"      # CPU\n",
        "    return torch.device(device_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VJUMYdKqVZxr",
        "outputId": "3c7a5fd5-4d39-4520-f9e1-85e11058e48a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using PyTorch with mps backend.\n",
            "Seeded everything with 42.\n"
          ]
        }
      ],
      "source": [
        "if not DEVICE:\n",
        "    DEVICE = get_best_device_for_pytorch()\n",
        "print(f\"Using PyTorch with {DEVICE} backend.\")\n",
        "\n",
        "seed_all(SEED)\n",
        "print(f\"Seeded everything with {SEED}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YAV-eWoVZxs"
      },
      "source": [
        "### MVTecDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FFDn_Y7xVZxt"
      },
      "outputs": [],
      "source": [
        "class MVTecDataset(Dataset[Tuple[torch.Tensor, int, torch.Tensor]]):\n",
        "    \"\"\"MVTec dataset of industrial objects with and without anomalies.\n",
        "\n",
        "    Yields (x, y, mask) tuples where:\n",
        "    - x is an RGB image from the class, as float tensor of shape (3, cropsize, cropsize);\n",
        "    - y is an int, 0 for good images, 1 for anomalous images;\n",
        "    - mask is 0 for normal pixels, 1 for anomalous pixels, as float tensor of shape (1, cropsize, cropsize).\n",
        "\n",
        "    Source: https://github.com/xiahaifeng1995/PaDiM-Anomaly-Detection-Localization-master/blob/main/datasets/mvtec.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset_path: Path, class_name: str = 'bottle',\n",
        "                 is_train: bool = True, resize: int = 256, cropsize: int = 224, return_only_indices=False):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.class_name = class_name\n",
        "        assert (dataset_path / class_name).is_dir(), f'Dataset class not found: {dataset_path / class_name}'\n",
        "        self.is_train = is_train\n",
        "\n",
        "        self.resize = resize\n",
        "        self.cropsize = cropsize\n",
        "\n",
        "        # load dataset\n",
        "        self.x, self.y, self.mask = self.load_dataset_folder()\n",
        "\n",
        "        # set transforms\n",
        "        self.transform_x = T.Compose([T.Resize(resize, Image.LANCZOS),\n",
        "                                      T.CenterCrop(cropsize),\n",
        "                                      T.ToTensor(),\n",
        "                                      T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                  std=[0.229, 0.224, 0.225])])\n",
        "        self.transform_mask = T.Compose([T.Resize(resize, Image.NEAREST),\n",
        "                                         T.CenterCrop(cropsize),\n",
        "                                         T.ToTensor()])\n",
        "\n",
        "        self.return_only_indices = return_only_indices\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int, torch.Tensor]:\n",
        "        if self.return_only_indices:  # Used for checking the answer of T1.1.\n",
        "            return idx\n",
        "\n",
        "        x, y, mask = self.x[idx], self.y[idx], self.mask[idx]\n",
        "\n",
        "        x = Image.open(x).convert('RGB')\n",
        "        x = cast(torch.Tensor, self.transform_x(x))\n",
        "\n",
        "        if y == 0:\n",
        "            mask = torch.zeros([1, self.cropsize, self.cropsize])\n",
        "        else:\n",
        "            assert mask is not None\n",
        "            mask = Image.open(mask)\n",
        "            mask = cast(torch.Tensor, self.transform_mask(mask))\n",
        "\n",
        "        return x, y, mask\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.x)\n",
        "\n",
        "    def load_dataset_folder(self) -> Tuple[List[Path], List[int], List[Optional[Path]]]:\n",
        "        phase = 'train' if self.is_train else 'test'\n",
        "        x: List[Path] = []\n",
        "        y: List[int] = []\n",
        "        mask: List[Optional[Path]] = []\n",
        "\n",
        "        img_dir = self.dataset_path / self.class_name / phase\n",
        "        gt_dir = self.dataset_path / self.class_name / 'ground_truth'\n",
        "\n",
        "        for img_type_dir in sorted(img_dir.iterdir()):\n",
        "            # Load images.\n",
        "            if not img_type_dir.is_dir():\n",
        "                continue\n",
        "            img_fpath_list = sorted(img_type_dir.glob('*.png'))\n",
        "            x.extend(img_fpath_list)\n",
        "\n",
        "            # Load ground-truth labels and masks.\n",
        "            if img_type_dir.name == 'good':\n",
        "                y.extend([0] * len(img_fpath_list))\n",
        "                mask.extend([None] * len(img_fpath_list))\n",
        "            else:\n",
        "                y.extend([1] * len(img_fpath_list))\n",
        "                mask.extend([gt_dir / img_type_dir.name / (f.stem + '_mask.png')\n",
        "                            for f in img_fpath_list])\n",
        "\n",
        "        assert len(x) == len(y) == len(mask), 'Number of x, y, and mask should be the same.'\n",
        "        return x, y, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jc3w7-nVVZxt"
      },
      "outputs": [],
      "source": [
        "def sample_idx(number_of_features: int, max_number_of_features: int) -> torch.Tensor:\n",
        "    assert number_of_features <= max_number_of_features\n",
        "    return torch.tensor(sample(range(0, max_number_of_features), number_of_features))\n",
        "\n",
        "\n",
        "def denormalization(x: FloatNDArray) -> NDArray[np.uint8]:\n",
        "    \"\"\"Denormalize with ImageNet values.\"\"\"\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    return (((x.transpose(1, 2, 0) * std) + mean) * 255.).astype(np.uint8)\n",
        "\n",
        "\n",
        "def embedding_concat(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Concatenate embeddings (along the channel dimension, upscaling y to match x).\n",
        "\n",
        "    Args:\n",
        "        x: Tensor of shape (B, C1, H1, W1).\n",
        "        y: Tensor of shape (B, C2, H2, W2).\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape is (B, C1 + C2, H1, W1).\n",
        "    \"\"\"\n",
        "    B, C1, H1, W1 = x.size()\n",
        "    _, C2, H2, W2 = y.size()\n",
        "    s = int(H1 / H2)\n",
        "    x = F.unfold(x, kernel_size=s, dilation=1, stride=s)\n",
        "    x = x.view(B, C1, s * s, H2, W2)\n",
        "    z = torch.zeros(B, C1 + C2, s * s, H2, W2).to(x.device)\n",
        "    for i in range(s * s):\n",
        "        z[:, :, i, :, :] = torch.cat((x[:, :, i, :, :], y), dim=1)\n",
        "    z = z.view(B, -1, H2 * W2)\n",
        "    z = F.fold(z, kernel_size=s, output_size=(H1, W1), stride=s)\n",
        "    return z\n",
        "\n",
        "def concatenate_embeddings_from_all_layers(layer_outputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
        "        embedding_vectors = layer_outputs['layer1']\n",
        "        for layer_name in ['layer2', 'layer3']:\n",
        "            embedding_vectors = embedding_concat(embedding_vectors, layer_outputs[layer_name])\n",
        "        return embedding_vectors\n",
        "\n",
        "def plot_fig(test_img, scores, gts, threshold: float, save_dir: Path, class_name: str):\n",
        "    num = len(scores)\n",
        "    vmax = scores.max() * 255.\n",
        "    vmin = scores.min() * 255.\n",
        "    for i in range(num):\n",
        "        img = test_img[i]\n",
        "        img = denormalization(img)\n",
        "        gt = gts[i].transpose(1, 2, 0).squeeze()\n",
        "        heat_map = scores[i] * 255\n",
        "        mask = scores[i]\n",
        "        mask[mask > threshold] = 1\n",
        "        mask[mask <= threshold] = 0\n",
        "        kernel = morphology.disk(4)\n",
        "        mask = morphology.opening(mask, kernel)\n",
        "        mask *= 255\n",
        "        vis_img = mark_boundaries(img, mask, color=(1, 0, 0), mode='thick')\n",
        "        fig_img, ax_img = plt.subplots(1, 5, figsize=(12, 3))\n",
        "        fig_img.subplots_adjust(right=0.9)\n",
        "        norm = colors.Normalize(vmin=vmin, vmax=vmax)\n",
        "        for ax_i in ax_img:\n",
        "            ax_i.axes.xaxis.set_visible(False)\n",
        "            ax_i.axes.yaxis.set_visible(False)\n",
        "        ax_img[0].imshow(img)\n",
        "        ax_img[0].title.set_text('Image')\n",
        "        ax_img[1].imshow(gt, cmap='gray')\n",
        "        ax_img[1].title.set_text('GroundTruth')\n",
        "        ax = ax_img[2].imshow(heat_map, cmap='jet', norm=norm)\n",
        "        ax_img[2].imshow(img, cmap='gray', interpolation='none')\n",
        "        ax_img[2].imshow(heat_map, cmap='jet', alpha=0.5, interpolation='none')\n",
        "        ax_img[2].title.set_text('Predicted heat map')\n",
        "        ax_img[3].imshow(mask, cmap='gray')\n",
        "        ax_img[3].title.set_text('Predicted mask')\n",
        "        ax_img[4].imshow(vis_img)\n",
        "        ax_img[4].title.set_text('Segmentation result')\n",
        "        left = 0.92\n",
        "        bottom = 0.15\n",
        "        width = 0.015\n",
        "        height = 1 - 2 * bottom\n",
        "        rect = [left, bottom, width, height]\n",
        "        cbar_ax = fig_img.add_axes(rect)\n",
        "        cb = plt.colorbar(ax, shrink=0.6, cax=cbar_ax, fraction=0.046)\n",
        "        cb.ax.tick_params(labelsize=8)\n",
        "        font = {\n",
        "            'family': 'serif',\n",
        "            'color': 'black',\n",
        "            'weight': 'normal',\n",
        "            'size': 8,\n",
        "        }\n",
        "        cb.set_label('Anomaly Score', fontdict=font)\n",
        "\n",
        "        fig_img.savefig(save_dir / f'{class_name}_{i}', dpi=100)\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VGs7EmL9VZxt"
      },
      "outputs": [],
      "source": [
        "def get_feature_extractor(arch: str) -> nn.Module:\n",
        "    if arch == 'resnet18':\n",
        "        model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1, progress=True)\n",
        "        # t_d = 448\n",
        "        # d = 40\n",
        "    elif arch == 'wide_resnet50_2':\n",
        "        model = wide_resnet50_2(weights=Wide_ResNet50_2_Weights.IMAGENET1K_V1, progress=True)\n",
        "        # t_d = 1792\n",
        "        # d = 550\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzRgYQYxVZxu"
      },
      "source": [
        "### PADIM class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "m5AzS1u1VZxu"
      },
      "outputs": [],
      "source": [
        "class PADIM():\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            backbone: str,\n",
        "            device: torch.device,\n",
        "            save_path: Path,\n",
        "            backbone_features_idx: torch.Tensor,\n",
        "            class_names: List[str] = [\"bottle\"],\n",
        "            plot_metrics: bool = False,\n",
        "    ) -> None:\n",
        "        self.arch = backbone\n",
        "        self.device = device\n",
        "        self.model = get_feature_extractor(backbone)\n",
        "        self.model.to(device)\n",
        "        self.model.eval()\n",
        "\n",
        "        self.feature_subset_indices = backbone_features_idx\n",
        "        self.feature_subset_indices.to(device)\n",
        "\n",
        "        self.outputs: Dict[str, torch.Tensor] = {}\n",
        "\n",
        "        self.class_names = class_names\n",
        "        self.save_path = save_path\n",
        "        self.plot_metrics = plot_metrics\n",
        "\n",
        "        self.setup_hooks()\n",
        "        (self.save_path / f'temp_{self.arch}').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.mean: FloatNDArray  # shape (C, H * W)\n",
        "        self.cov: FloatNDArray  # shape (C, C, H * W)\n",
        "\n",
        "    def setup_hooks(self):\n",
        "        \"\"\"Setup hooks to store model's intermediate outputs.\"\"\"\n",
        "        self.model.layer1[-1].register_forward_hook(lambda _, __, x: self.outputs.update({'layer1': x}))\n",
        "        self.model.layer2[-1].register_forward_hook(lambda _, __, x: self.outputs.update({'layer2': x}))\n",
        "        self.model.layer3[-1].register_forward_hook(lambda _, __, x: self.outputs.update({'layer3': x}))\n",
        "\n",
        "    def train_and_test(self, train_dataloader: DataLoader, test_dataloader: DataLoader) -> float:\n",
        "        self.train(train_dataloader)\n",
        "        return self.test(test_dataloader)\n",
        "\n",
        "    def train(self, train_dataloader: DataLoader) -> None:\n",
        "        self.train_outputs: Dict[str, List[torch.Tensor]] = {'layer1': [], 'layer2': [], 'layer3': []}\n",
        "        for x, _, _ in tqdm(train_dataloader, desc='Feature extraction (train)'):\n",
        "            # Run model prediction.\n",
        "            with torch.no_grad():\n",
        "                _ = self.model(x.to(DEVICE))\n",
        "            # Get intermediate layer outputs.\n",
        "            assert list(self.outputs.keys())  == ['layer1', 'layer2', 'layer3'], list(self.outputs.keys())\n",
        "            for k, v in self.outputs.items():\n",
        "                self.train_outputs[k].append(v.cpu().detach())\n",
        "            # Reset hook outputs.\n",
        "            self.outputs = {}\n",
        "\n",
        "        embedding_vectors = concatenate_embeddings_from_all_layers(\n",
        "            {k: torch.cat(v, 0) for k, v in self.train_outputs.items()})\n",
        "        embedding_vectors_subset = torch.index_select(embedding_vectors, 1, self.feature_subset_indices.cpu())\n",
        "\n",
        "        self.mean, self.cov = self.estimate_multivariate_gaussian(embedding_vectors_subset)\n",
        "        del(self.train_outputs)\n",
        "\n",
        "    def estimate_multivariate_gaussian(self, embedding_vectors: torch.Tensor\n",
        "                                       ) -> Tuple[FloatNDArray, FloatNDArray]:\n",
        "        \"\"\"Calculates multivariate Gaussian distribution.\n",
        "\n",
        "        Takes embeddings of shape (N, C, H, W).\n",
        "        Returns (mean, covariance) of shape (C, H * W) and (C, C, H * W) respectively.\n",
        "        \"\"\"\n",
        "        B, C, H, W = embedding_vectors.size()\n",
        "        embedding_vectors = embedding_vectors.view(B, C, H * W)\n",
        "        mean = torch.mean(embedding_vectors, dim=0).numpy()\n",
        "        cov = torch.zeros(C, C, H * W).numpy()\n",
        "        I = np.identity(C)\n",
        "        for i in tqdm(range(H * W), desc=\"Covariance estimation\"):\n",
        "            cov[:, :, i] = np.cov(embedding_vectors[:, :, i].numpy(), rowvar=False) + 0.01 * I\n",
        "        return mean, cov\n",
        "\n",
        "    def test(self, test_dataloader: DataLoader) -> float:\n",
        "        self.test_outputs: Dict[str, List[torch.Tensor]] = {'layer1': [], 'layer2': [], 'layer3': []}\n",
        "        test_imgs: List[FloatNDArray] = []\n",
        "        gt_list: List[NDArray[Any]] = []\n",
        "        gt_mask_list: List[FloatNDArray] = []\n",
        "\n",
        "        # Extract test set features.\n",
        "        for x, y, mask in tqdm(test_dataloader, desc='Feature extraction (test)', disable=False):\n",
        "            x_shape = x.shape\n",
        "            test_imgs.extend(x.cpu().detach().numpy())\n",
        "            gt_list.extend(y.cpu().detach().numpy())\n",
        "            gt_mask_list.extend(mask.cpu().detach().numpy())\n",
        "            # Run model prediction.\n",
        "            with torch.no_grad():\n",
        "                _ = self.model(x.to(DEVICE))\n",
        "            # Get intermediate layer outputs.\n",
        "            assert list(self.outputs.keys())  == ['layer1', 'layer2', 'layer3']\n",
        "            for k, v in self.outputs.items():\n",
        "                self.test_outputs[k].append(v.cpu().detach())\n",
        "            # Reset hook outputs.\n",
        "            self.outputs = {}\n",
        "        gt_mask = np.asarray(gt_mask_list)  # shape (len(test_dataset), 1, H, W)\n",
        "\n",
        "        embedding_vectors = concatenate_embeddings_from_all_layers(\n",
        "            {k: torch.cat(v, 0) for k, v in self.test_outputs.items()})\n",
        "        # shape (len(test_dataset), len(feature_subset_indices), H1, W1)\n",
        "        embedding_vectors_subset = torch.index_select(embedding_vectors, 1, self.feature_subset_indices.cpu())\n",
        "\n",
        "        distances = self.calculate_distances(embedding_vectors_subset)\n",
        "        score_map = self.prepare_anomaly_map((x_shape[2], x_shape[3]), distances)\n",
        "\n",
        "        img_fpr, img_tpr, img_auroc = self.calculate_auroc_image_level(score_map, gt_list)\n",
        "        pxl_fpr, pxl_tpr, pxl_auroc = self.calculate_auroc_pixel_level(score_map, gt_mask)\n",
        "\n",
        "        if self.plot_metrics:\n",
        "            print(f'[TEST] Image AUROC: {img_auroc:.3f}')\n",
        "            print(f'[TEST] Pixel AUROC: {pxl_auroc:.3f}')\n",
        "            threshold = self.calculate_optimal_threshold(score_map, gt_mask)\n",
        "            self.plot_test_results_for_class(gt_mask_list, test_imgs, score_map, threshold, img_fpr, img_tpr, img_auroc, pxl_fpr, pxl_tpr, pxl_auroc)\n",
        "\n",
        "        return pxl_auroc\n",
        "\n",
        "    # TODO: Some of your code for Task 1 goes here. You can add more functions if needed, but use the ones below - we will use them for checking your solution.\n",
        "    def test_permutation_importance(self, val_dataloader: DataLoader, features_to_permute: List[int]) -> List[float]:\n",
        "        \"\"\"Runs a series of tests on `val_dataloader`.\n",
        "        Returns a list of pixelwise AUROCs, where the n-th element of the list is generated by testing the embeddings from `permute_feature(embeddings, features_to_permute[n]).\"\"\"\n",
        "        pass\n",
        "\n",
        "    def permute_feature(self, embedding_vectors_subset: torch.Tensor, number_of_feature_to_permute: int) -> torch.Tensor:\n",
        "        \"\"\"Permutes the embeddings.\n",
        "\n",
        "        Takes embeddings of shape (N, C, H, W) and feature number to permute.\n",
        "        Returns embeddings with the same shape. See the description of T1 for the details.\n",
        "        \"\"\"\n",
        "        pass\n",
        "    # TODO: End of your code for Task 1 (here)\n",
        "\n",
        "    def plot_test_results_for_class(self, gt_mask_list, test_imgs,\n",
        "                                    score_map, threshold: float,\n",
        "                                    img_fpr, img_tpr, img_auroc: float,\n",
        "                                    pxl_fpr, pxl_tpr, pxl_auroc: float):\n",
        "        _, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
        "        ax[0].plot(img_fpr, img_tpr, label=f'Image AUROC: {img_auroc:.3f}')\n",
        "        ax[1].plot(pxl_fpr, pxl_tpr, label=f'Pixel AUROC: {pxl_auroc:.3f}')\n",
        "\n",
        "        save_dir = self.save_path / f'pictures_{self.arch}'\n",
        "        save_dir.mkdir(parents=True, exist_ok=True)\n",
        "        plot_fig(test_imgs, score_map, gt_mask_list,\n",
        "                 threshold, save_dir, \"\")\n",
        "\n",
        "    def calculate_auroc_image_level(self, score_map: FloatNDArray, gt_list: List[NDArray[Any]]) -> Tuple[FloatNDArray, FloatNDArray, float]:\n",
        "        \"\"\"Calculate image-level AUROC score.\"\"\"\n",
        "        img_scores = score_map.reshape(score_map.shape[0], -1).max(axis=1)\n",
        "        fpr, tpr, _ = roc_curve(gt_list, img_scores)  # false-positive-rates and true-positive-rates for consecutive thresholds (for plotting).\n",
        "        img_auroc = roc_auc_score(gt_list, img_scores)\n",
        "        return fpr, tpr, float(img_auroc)\n",
        "\n",
        "    def calculate_auroc_pixel_level(self, score_map: FloatNDArray, gt_mask: FloatNDArray) -> Tuple[FloatNDArray, FloatNDArray, float]:\n",
        "        \"\"\"Calculate per-pixel level AUROC.\"\"\"\n",
        "        assert score_map.shape == gt_mask.squeeze().shape, f\"{score_map.shape=}, {gt_mask.shape=}\"\n",
        "        fpr, tpr, _ = roc_curve(gt_mask.flatten(), score_map.flatten())\n",
        "        per_pixel_auroc = roc_auc_score(gt_mask.flatten(), score_map.flatten())\n",
        "        return fpr, tpr, float(per_pixel_auroc)\n",
        "\n",
        "    def calculate_optimal_threshold(self, score_map: FloatNDArray, gt_mask: FloatNDArray) -> float:\n",
        "        \"\"\"Calculate the optimal threshold with regard to F1 score.\"\"\"\n",
        "        assert score_map.shape == gt_mask.squeeze().shape\n",
        "        precision, recall, thresholds = precision_recall_curve(\n",
        "            gt_mask.flatten(), score_map.flatten())\n",
        "        a = 2 * precision * recall\n",
        "        b = precision + recall\n",
        "        f1 = np.divide(a, b, out=np.zeros_like(a), where=(b != 0))\n",
        "        threshold = thresholds[np.argmax(f1)]\n",
        "        return threshold\n",
        "\n",
        "    def calculate_distances(self, embedding_vectors: torch.Tensor) -> FloatNDArray:\n",
        "        \"\"\"Calculate Mahalanobis distance of each embedding vector from self.mean.\n",
        "\n",
        "        For embeddings of shape (N, C, H, W), returns shape (N, H, W).\n",
        "        \"\"\"\n",
        "        B, C, H, W = embedding_vectors.size()\n",
        "        embedding_vectors = embedding_vectors.view(B, C, H * W).numpy()\n",
        "        dist_list: List[List[np.float64]] = []\n",
        "        for i in range(H * W):\n",
        "            mean = self.mean[:, i]\n",
        "            conv_inv = np.linalg.inv(self.cov[:, :, i])\n",
        "            dist = [mahalanobis(sample[:, i], mean, conv_inv)\n",
        "                    for sample in embedding_vectors]\n",
        "            dist_list.append(dist)\n",
        "\n",
        "        return np.array(dist_list).transpose(1, 0).reshape(B, H, W)\n",
        "\n",
        "    def prepare_anomaly_map(self, shape: Tuple[int, int], distances: FloatNDArray) -> FloatNDArray:\n",
        "        \"\"\"Upsample distances to `shape`, apply Gaussian smoothing, and normalize to [0,1].\n",
        "\n",
        "        For distances of shape (N, H, W) and `shape` equal to (H2, W2), returns shape (N, H2, W2).\n",
        "        \"\"\"\n",
        "        dists = torch.Tensor(distances).unsqueeze(1)\n",
        "        shape = (dists.shape[0],) + shape\n",
        "        score_map = cast(FloatNDArray, F.interpolate(\n",
        "            dists, size=shape[2], mode='bilinear', align_corners=False).squeeze().numpy())\n",
        "        for i in range(score_map.shape[0]):\n",
        "            score_map[i] = gaussian_filter(score_map[i], sigma=4)\n",
        "\n",
        "        min_score, max_score = score_map.min(), score_map.max()\n",
        "        return (score_map - min_score) / (max_score - min_score + 1e-10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BufmMzabVZxu"
      },
      "source": [
        "### Let's see whether it works.\n",
        "Take a look to the `SAVE_PATH` to inspect the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HlfOLsv3VZxv",
        "outputId": "b24cb2fe-87a7-43c3-a87b-df8355b3f44b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== bottle\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feature extraction (train): 100%|██████████| 105/105 [00:07<00:00, 13.56it/s]\n",
            "Covariance estimation: 100%|██████████| 3136/3136 [00:00<00:00, 5210.01it/s]\n",
            "Feature extraction (test): 100%|██████████| 42/42 [00:03<00:00, 12.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TEST] Image AUROC: 0.998\n",
            "[TEST] Pixel AUROC: 0.981\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "seed_all(SEED)\n",
        "CLASS_NAMES = [\n",
        "            'bottle', #'metal_nut'\n",
        "            # 'cable', 'capsule', 'carpet', 'grid', 'hazelnut', 'leather',\n",
        "            # 'pill', 'screw', 'tile', 'toothbrush', 'transistor', 'wood', 'zipper'\n",
        "        ]\n",
        "BATCH_SIZE = 2\n",
        "RESIZE = 256 * 1\n",
        "CROP_SIZE = 224 * 1\n",
        "BACKBONE = \"resnet18\"\n",
        "NUMBER_OF_BACKBONE_FEATURES = 50\n",
        "MAX_NUMBER_OF_BACKBONE_FEATURES = 448\n",
        "\n",
        "run_timestamp = time.time()\n",
        "for class_name in CLASS_NAMES:\n",
        "    print('=' * 10, class_name)\n",
        "    SAVE_PATH = Path(f\"./results/{run_timestamp}/{class_name}\")\n",
        "\n",
        "    train_dataset = MVTecDataset(DATA_PATH, class_name=class_name, is_train=True, resize=RESIZE, cropsize=CROP_SIZE)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
        "    test_dataset = MVTecDataset(DATA_PATH, class_name=class_name, is_train=False, resize=RESIZE, cropsize=CROP_SIZE)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
        "\n",
        "    padim = PADIM(\n",
        "        backbone=BACKBONE,\n",
        "        device=DEVICE,\n",
        "        backbone_features_idx=sample_idx(NUMBER_OF_BACKBONE_FEATURES, MAX_NUMBER_OF_BACKBONE_FEATURES),\n",
        "        save_path=SAVE_PATH,\n",
        "        plot_metrics=True,\n",
        "    )\n",
        "\n",
        "    padim.train_and_test(\n",
        "        train_dataloader=train_dataloader,\n",
        "        test_dataloader=test_dataloader,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL00CE8iVZxw"
      },
      "source": [
        "# Task 1. Finding the right features (40%)\n",
        "\n",
        "The authors of the paper argue that it doesn't really matter how we choose a subset of features. Let's make some steps towards exploring whether it's true for three different classes (`bottle`, `transistor`, `metal_nut`).\n",
        "Design an experiment which will rank the ResNet18 features by its importance. To do so, we'll implement our variation of [permutation feature importance](https://scikit-learn.org/stable/modules/permutation_importance.html#outline-of-the-permutation-importance-algorithm) on a subset of features produced by the backbone.\n",
        "\n",
        "## 1.1 Preparing the data\n",
        "- Using the test dataset, create `val_dataloader` (every even sample from the original test dataset) and `test_dataloader` (every odd sample). `SubsetRandomSampler` might be handy here.\n",
        "- Then, create 3-fold cross validation-like process in which you'll train three PADIM models on the first 100 ResNet features in three equally sized subsets of train dataset in which you discard 1/3 of the data ($\\texttt{padim}_{k}\\texttt{.train}(\\texttt{train\\_dataloader}_k)$) (see below). Again, `SubsetRandomSampler` might be handy here.\n",
        "\n",
        "In other words, you should have:\n",
        "\n",
        "- for $k=0$, the first 10 images indexes from the train dataset we should train on are `[1, 2, 4, 5, 7, 8, 10, 11, 13, 14]`,\n",
        "- for $k=1$, that's `[0, 2, 3, 5, 6, 8, 9, 11, 12, 14]`,\n",
        "- and for $k=2$, that's `[0, 1, 3, 4, 6, 7, 9, 10, 12, 13]`.\n",
        "\n",
        "For val and train, you should have `[0, 2, 4, ...]` and `[1, 3, 5, ...]` respectively (from the test dataset).\n",
        "\n",
        "Don't worry about the sampling order.\n",
        "Use these names for DataLoaders `val_dataloader`, `test_dataloader`. For k-fold training, store dataloaders in `train_dataloaders: List[DataLoader]`, where each element represent different $k$.\n",
        "For each class, store the results in `dataloaders` dictionary (the variable is defined in the code below) - we will use this to check your solution.\n",
        "\n",
        "## 1.2 Calculating the importances\n",
        "- In a given fold, each $j$-th feature shall be ranked based on the pixel-wise AUROC difference between the output of that model ($s_{k} \\leftarrow \\texttt{padim}_{k}\\texttt{.test}(\\texttt{val\\_dataloader})$) and the output with the model with permuted $j$-th feature ($s_{k, j} \\leftarrow \\texttt{padim}_{k}\\texttt{.test\\_permutation\\_importance}(\\texttt{val\\_dataloader, features\\_to\\_permute=}[j])$). In practice you can pass all the numbers of features to permute (instead of 1-element list and do the loop inside the method. See also `test_permutation_importance` method stub above.\n",
        "- Implement `permute_feature` method as follows: given the tensor with embeddings with shape `[B, C, H, W]`, by permutation of the $j$-th feature we mean randomly swapped values for $C=j$. Although (ideally) the order of swapping shall be **different** for every image, we don't require you to strictly guarantee that you won't get the same permutation twice (what matters here is not using the same permutation for **every** sample - you can e.g. use distinct calls to a shuffling function for every sample). In other words, for every image $b$ and feature $j$ you need to shuffle the last two dimensions (marked as stars in `[b, j, *, *]`) in an (ideally) unique manner.\n",
        "- Then, calculate the mean importance $i$ averaged on these folds and plot weights importance for the class ($i_j \\leftarrow \\frac{1}{K} \\sum_{k} ( s_k -  s_{k, j} )$, where $K$ is the number of folds).\n",
        "- Append results in `results` dictionary, where keys are class names and values are the lists of averaged feature importances (from feature 0 to feature 99).\n",
        "\n",
        "## 1.3 Drawing conclusions\n",
        "\n",
        "- Finally, for every class train three models on the full training data and evaluate it on the `test_dataloader`. The first model shall use the first 10 features, the second shall use worst 10 features (in terms of feature importance), and the third shall contain the best 10 features.\n",
        "- Write your conclusions (with the things enlisted below in mind). Simply plotting charts or outputting logs without any comment doesn't qualify as an answer to a question.\n",
        "\n",
        "Note 1: Limit yourself to the first 100 features of ResNet18. If you want, you can go with all of available features instead of 100, but it'll take some time to calculate. Converting parts of the code to PyTorch and running on GPU might change a lot here, but this is not evaluated in this exercise. This experiment can be calculated without GPU in less than one hour anyway.\n",
        "\n",
        "Note 2: If you'd like to be fully covered, one needs to explore if the features are correlated, as this might bias the results of feature importance calculations. However, this is not evaluated in this task for the sake of simplicity (that is, examining the 100 first features without worrying about correlated features are enough to get 100% from this task)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LzYc9ExXVZxw"
      },
      "outputs": [],
      "source": [
        "# do not modify\n",
        "CLASS_NAMES = ['bottle', 'transistor', 'metal_nut']\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "RESIZE = 256 * 2 // 4\n",
        "CROP_SIZE = 224 * 2 // 4\n",
        "BACKBONE = \"resnet18\"\n",
        "NUMBER_OF_BACKBONE_FEATURES = 10\n",
        "MAX_NUMBER_OF_BACKBONE_FEATURES = 100  # 448\n",
        "folds = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3UHb3HymVZxx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1702816242.7805471\n"
          ]
        }
      ],
      "source": [
        "seed_all(SEED)\n",
        "results = {c: [0] * MAX_NUMBER_OF_BACKBONE_FEATURES for c in CLASS_NAMES}\n",
        "\n",
        "run_timestamp = time.time()\n",
        "print(f\"{run_timestamp}\")\n",
        "\n",
        "idx_all_fatures = torch.Tensor(range(MAX_NUMBER_OF_BACKBONE_FEATURES)).int()\n",
        "idx_first_n_features = torch.Tensor(range(NUMBER_OF_BACKBONE_FEATURES)).int()\n",
        "\n",
        "dataloaders = {c: {\"val_dataloader\": None, \"test_dataloader\": None, \"train_dataloaders\": None} for c in CLASS_NAMES}\n",
        "\n",
        "# TODO: Your code for T1.1, T1.2, and T1.3 goes below. Don't forget to write `test_permutation_importance` and `permute_feature` above in the PADIM code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# T1.1\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "\n",
        "\n",
        "test_dataset = MVTecDataset(DATA_PATH, class_name=class_name, is_train=False, resize=RESIZE, cropsize=CROP_SIZE)\n",
        "train_dataset = MVTecDataset(DATA_PATH, class_name=class_name, is_train=True, resize=RESIZE, cropsize=CROP_SIZE)\n",
        "\n",
        "for class_name in CLASS_NAMES:\n",
        "    odd_indices = list(range(len(test_dataset)))[::2]\n",
        "    even_indices = list(range(len(test_dataset)))[1::2]\n",
        "\n",
        "    val_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True, \n",
        "                                sampler=SubsetRandomSampler(even_indices))\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True,\n",
        "                                sampler=SubsetRandomSampler(odd_indices))\n",
        "    \n",
        "    train_dataloaders = []\n",
        "    for fold in range(folds):\n",
        "        train_indices = [idx for idx in list(range(len(train_dataset))) if idx % folds != fold]\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True,\n",
        "                                    sampler=SubsetRandomSampler(train_indices))\n",
        "        train_dataloaders.append(train_dataloader)\n",
        "\n",
        "    dataloaders[class_name] = {\"val_dataloader\": val_dataloader, \"test_dataloader\": test_dataloader, \"train_dataloaders\": train_dataloaders}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "za2w3WO3VZxx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 63, 73, 72, 71, 70, 69, 68, 67, 66]\n",
            "[0, 72, 71, 70, 69, 68, 67, 66, 65, 64]\n",
            "[0, 63, 73, 72, 71, 70, 69, 68, 67, 66]\n",
            "[0, 72, 71, 70, 69, 68, 67, 66, 65, 64]\n",
            "[0, 63, 73, 72, 71, 70, 69, 68, 67, 66]\n",
            "[0, 72, 71, 70, 69, 68, 67, 66, 65, 64]\n",
            "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99, 101, 103, 105, 107, 109, 111, 113]\n",
            "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114]\n",
            "[1, 2, 4, 5, 7, 8, 10, 11, 13, 14, 16, 17, 19, 20, 22, 23, 25, 26, 28, 29, 31, 32, 34, 35, 37, 38, 40, 41, 43, 44, 46, 47, 49, 50, 52, 53, 55, 56, 58, 59, 61, 62, 64, 65, 67, 68, 70, 71, 73, 74, 76, 77, 79, 80, 82, 83, 85, 86, 88, 89, 91, 92, 94, 95, 97, 98, 100, 101, 103, 104, 106, 107, 109, 110, 112, 113, 115, 116, 118, 119, 121, 122, 124, 125, 127, 128, 130, 131, 133, 134, 136, 137, 139, 140, 142, 143, 145, 146, 148, 149, 151, 152, 154, 155, 157, 158, 160, 161, 163, 164, 166, 167, 169, 170, 172, 173, 175, 176, 178, 179, 181, 182, 184, 185, 187, 188, 190, 191, 193, 194, 196, 197, 199, 200, 202, 203, 205, 206, 208, 209, 211, 212, 214, 215, 217, 218]\n",
            "[0, 2, 3, 5, 6, 8, 9, 11, 12, 14, 15, 17, 18, 20, 21, 23, 24, 26, 27, 29, 30, 32, 33, 35, 36, 38, 39, 41, 42, 44, 45, 47, 48, 50, 51, 53, 54, 56, 57, 59, 60, 62, 63, 65, 66, 68, 69, 71, 72, 74, 75, 77, 78, 80, 81, 83, 84, 86, 87, 89, 90, 92, 93, 95, 96, 98, 99, 101, 102, 104, 105, 107, 108, 110, 111, 113, 114, 116, 117, 119, 120, 122, 123, 125, 126, 128, 129, 131, 132, 134, 135, 137, 138, 140, 141, 143, 144, 146, 147, 149, 150, 152, 153, 155, 156, 158, 159, 161, 162, 164, 165, 167, 168, 170, 171, 173, 174, 176, 177, 179, 180, 182, 183, 185, 186, 188, 189, 191, 192, 194, 195, 197, 198, 200, 201, 203, 204, 206, 207, 209, 210, 212, 213, 215, 216, 218, 219]\n",
            "[0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 15, 16, 18, 19, 21, 22, 24, 25, 27, 28, 30, 31, 33, 34, 36, 37, 39, 40, 42, 43, 45, 46, 48, 49, 51, 52, 54, 55, 57, 58, 60, 61, 63, 64, 66, 67, 69, 70, 72, 73, 75, 76, 78, 79, 81, 82, 84, 85, 87, 88, 90, 91, 93, 94, 96, 97, 99, 100, 102, 103, 105, 106, 108, 109, 111, 112, 114, 115, 117, 118, 120, 121, 123, 124, 126, 127, 129, 130, 132, 133, 135, 136, 138, 139, 141, 142, 144, 145, 147, 148, 150, 151, 153, 154, 156, 157, 159, 160, 162, 163, 165, 166, 168, 169, 171, 172, 174, 175, 177, 178, 180, 181, 183, 184, 186, 187, 189, 190, 192, 193, 195, 196, 198, 199, 201, 202, 204, 205, 207, 208, 210, 211, 213, 214, 216, 217, 219]\n",
            "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99, 101, 103, 105, 107, 109, 111, 113]\n",
            "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114]\n",
            "[1, 2, 4, 5, 7, 8, 10, 11, 13, 14, 16, 17, 19, 20, 22, 23, 25, 26, 28, 29, 31, 32, 34, 35, 37, 38, 40, 41, 43, 44, 46, 47, 49, 50, 52, 53, 55, 56, 58, 59, 61, 62, 64, 65, 67, 68, 70, 71, 73, 74, 76, 77, 79, 80, 82, 83, 85, 86, 88, 89, 91, 92, 94, 95, 97, 98, 100, 101, 103, 104, 106, 107, 109, 110, 112, 113, 115, 116, 118, 119, 121, 122, 124, 125, 127, 128, 130, 131, 133, 134, 136, 137, 139, 140, 142, 143, 145, 146, 148, 149, 151, 152, 154, 155, 157, 158, 160, 161, 163, 164, 166, 167, 169, 170, 172, 173, 175, 176, 178, 179, 181, 182, 184, 185, 187, 188, 190, 191, 193, 194, 196, 197, 199, 200, 202, 203, 205, 206, 208, 209, 211, 212, 214, 215, 217, 218]\n",
            "[0, 2, 3, 5, 6, 8, 9, 11, 12, 14, 15, 17, 18, 20, 21, 23, 24, 26, 27, 29, 30, 32, 33, 35, 36, 38, 39, 41, 42, 44, 45, 47, 48, 50, 51, 53, 54, 56, 57, 59, 60, 62, 63, 65, 66, 68, 69, 71, 72, 74, 75, 77, 78, 80, 81, 83, 84, 86, 87, 89, 90, 92, 93, 95, 96, 98, 99, 101, 102, 104, 105, 107, 108, 110, 111, 113, 114, 116, 117, 119, 120, 122, 123, 125, 126, 128, 129, 131, 132, 134, 135, 137, 138, 140, 141, 143, 144, 146, 147, 149, 150, 152, 153, 155, 156, 158, 159, 161, 162, 164, 165, 167, 168, 170, 171, 173, 174, 176, 177, 179, 180, 182, 183, 185, 186, 188, 189, 191, 192, 194, 195, 197, 198, 200, 201, 203, 204, 206, 207, 209, 210, 212, 213, 215, 216, 218, 219]\n",
            "[0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 15, 16, 18, 19, 21, 22, 24, 25, 27, 28, 30, 31, 33, 34, 36, 37, 39, 40, 42, 43, 45, 46, 48, 49, 51, 52, 54, 55, 57, 58, 60, 61, 63, 64, 66, 67, 69, 70, 72, 73, 75, 76, 78, 79, 81, 82, 84, 85, 87, 88, 90, 91, 93, 94, 96, 97, 99, 100, 102, 103, 105, 106, 108, 109, 111, 112, 114, 115, 117, 118, 120, 121, 123, 124, 126, 127, 129, 130, 132, 133, 135, 136, 138, 139, 141, 142, 144, 145, 147, 148, 150, 151, 153, 154, 156, 157, 159, 160, 162, 163, 165, 166, 168, 169, 171, 172, 174, 175, 177, 178, 180, 181, 183, 184, 186, 187, 189, 190, 192, 193, 195, 196, 198, 199, 201, 202, 204, 205, 207, 208, 210, 211, 213, 214, 216, 217, 219]\n",
            "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99, 101, 103, 105, 107, 109, 111, 113]\n",
            "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114]\n",
            "[1, 2, 4, 5, 7, 8, 10, 11, 13, 14, 16, 17, 19, 20, 22, 23, 25, 26, 28, 29, 31, 32, 34, 35, 37, 38, 40, 41, 43, 44, 46, 47, 49, 50, 52, 53, 55, 56, 58, 59, 61, 62, 64, 65, 67, 68, 70, 71, 73, 74, 76, 77, 79, 80, 82, 83, 85, 86, 88, 89, 91, 92, 94, 95, 97, 98, 100, 101, 103, 104, 106, 107, 109, 110, 112, 113, 115, 116, 118, 119, 121, 122, 124, 125, 127, 128, 130, 131, 133, 134, 136, 137, 139, 140, 142, 143, 145, 146, 148, 149, 151, 152, 154, 155, 157, 158, 160, 161, 163, 164, 166, 167, 169, 170, 172, 173, 175, 176, 178, 179, 181, 182, 184, 185, 187, 188, 190, 191, 193, 194, 196, 197, 199, 200, 202, 203, 205, 206, 208, 209, 211, 212, 214, 215, 217, 218]\n",
            "[0, 2, 3, 5, 6, 8, 9, 11, 12, 14, 15, 17, 18, 20, 21, 23, 24, 26, 27, 29, 30, 32, 33, 35, 36, 38, 39, 41, 42, 44, 45, 47, 48, 50, 51, 53, 54, 56, 57, 59, 60, 62, 63, 65, 66, 68, 69, 71, 72, 74, 75, 77, 78, 80, 81, 83, 84, 86, 87, 89, 90, 92, 93, 95, 96, 98, 99, 101, 102, 104, 105, 107, 108, 110, 111, 113, 114, 116, 117, 119, 120, 122, 123, 125, 126, 128, 129, 131, 132, 134, 135, 137, 138, 140, 141, 143, 144, 146, 147, 149, 150, 152, 153, 155, 156, 158, 159, 161, 162, 164, 165, 167, 168, 170, 171, 173, 174, 176, 177, 179, 180, 182, 183, 185, 186, 188, 189, 191, 192, 194, 195, 197, 198, 200, 201, 203, 204, 206, 207, 209, 210, 212, 213, 215, 216, 218, 219]\n",
            "[0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 15, 16, 18, 19, 21, 22, 24, 25, 27, 28, 30, 31, 33, 34, 36, 37, 39, 40, 42, 43, 45, 46, 48, 49, 51, 52, 54, 55, 57, 58, 60, 61, 63, 64, 66, 67, 69, 70, 72, 73, 75, 76, 78, 79, 81, 82, 84, 85, 87, 88, 90, 91, 93, 94, 96, 97, 99, 100, 102, 103, 105, 106, 108, 109, 111, 112, 114, 115, 117, 118, 120, 121, 123, 124, 126, 127, 129, 130, 132, 133, 135, 136, 138, 139, 141, 142, 144, 145, 147, 148, 150, 151, 153, 154, 156, 157, 159, 160, 162, 163, 165, 166, 168, 169, 171, 172, 174, 175, 177, 178, 180, 181, 183, 184, 186, 187, 189, 190, 192, 193, 195, 196, 198, 199, 201, 202, 204, 205, 207, 208, 210, 211, 213, 214, 216, 217, 219]\n"
          ]
        }
      ],
      "source": [
        "# Run at the end, but do not modify - we will use this to asses your output.\n",
        "for c in CLASS_NAMES:\n",
        "    s = pd.Series(results[c])\n",
        "    print(s.sort_values(ascending=False)[:10].index.tolist())\n",
        "    print(s.sort_values(ascending=True)[:10].index.tolist())\n",
        "\n",
        "def get_sorted_indices(loader):\n",
        "    loader.dataset.return_only_indices = True\n",
        "    indices = sorted([x.item() for x in loader])\n",
        "    loader.dataset.return_only_indices = False\n",
        "    return indices\n",
        "\n",
        "for c in CLASS_NAMES:\n",
        "    print(get_sorted_indices(dataloaders[c][\"val_dataloader\"]))\n",
        "    print(get_sorted_indices(dataloaders[c][\"test_dataloader\"]))\n",
        "    for v in dataloaders[c][\"train_dataloaders\"]:\n",
        "        print(get_sorted_indices(v))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qtn9ylbVZxx"
      },
      "source": [
        "# Task 2. Improving PADIM with Online Covariance Estimation\n",
        "\n",
        "This implementation of PADIM can be improved in numerous ways. In this exercise, you'll try to indicate its shortcomings and provide some means to mitigate them.\n",
        "\n",
        "#### 2.1. PADIM's training complexity (15%)\n",
        "\n",
        "- Identify the key operations contributing to the algorithm's training space complexity *in this implementation*. Don't focus on the backbone, as it is not the part of the algorithm (however, its output is).\n",
        "- Shortly discuss the implications for scalability. You can support your claims by charts if needed.\n",
        "\n",
        "*Hint: this doesn't need to be super formal analysis - it's about fiding the \"worst\" parts of this implementation. You can support your claims with a chart and brief description (e.g. \"X dominates the complexity, as it's quadratic.\")*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdODnlnsVZxx"
      },
      "source": [
        "```Your answer to task 2.1 goes here```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD9esI1fVZxx"
      },
      "outputs": [],
      "source": [
        "# Your code goes here (if needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by7h9sp3VZxx"
      },
      "source": [
        "#### 2.2 Online mean and covariance (35%)\n",
        "Implement a PyTorch version of [online covariance matrix estimation](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online) in the training as an alternative to the current method in PADIM.\n",
        "Calculate the mean in an online fashion as well.\n",
        "Your implementation shall run on the selected `torch.device` (such as GPU).\n",
        "No need to reimplement the testing routine to online in this exercise (although it'd be nice to have for Task 1), albeit small changes might be necessary (such as conversion from `torch.Tensor` to `np.ndarray`).\n",
        "\n",
        "Passing criteria:\n",
        "```python\n",
        "torch.allclose(padim_online.mean, torch.Tensor(padim_offline.mean).to(DEVICE), atol=0.01)\n",
        "torch.allclose(padim_online.cov, torch.Tensor(padim_offline.cov).to(DEVICE), atol=0.01)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-1wMyouVZxx"
      },
      "outputs": [],
      "source": [
        "class PADIMWithOnlineCovariance(PADIM):\n",
        "\n",
        "    ### TODO: Your code goes here\n",
        "    def __init__(\n",
        "            self,\n",
        "            backbone: str,\n",
        "            device: torch.device,\n",
        "            save_path: Path,\n",
        "            backbone_features_idx: List[int],\n",
        "            class_names=...,\n",
        "            plot_metrics=False,\n",
        "            ) -> None:\n",
        "        super().__init__(backbone, device, save_path, backbone_features_idx, class_names, plot_metrics)\n",
        "\n",
        "    def train(self, train_dataloader: DataLoader, C: int, H: int, W: int):\n",
        "        \"\"\"C, H, W come from the size of embeddings: [B, C, H, W]\"\"\"\n",
        "        pass\n",
        "\n",
        "    ### END OF YOUR CODE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TT0JEi5VZxx"
      },
      "outputs": [],
      "source": [
        "# do not modify\n",
        "seed_all(SEED)\n",
        "class_name = 'bottle'\n",
        "BATCH_SIZE = 1\n",
        "RESIZE = 256 * 1\n",
        "CROP_SIZE = 224 * 1\n",
        "BACKBONE = \"resnet18\"\n",
        "NUMBER_OF_BACKBONE_FEATURES = 30\n",
        "MAX_NUMBER_OF_BACKBONE_FEATURES = 448\n",
        "# DEVICE=\"cpu\"\n",
        "\n",
        "indices = sample_idx(NUMBER_OF_BACKBONE_FEATURES, MAX_NUMBER_OF_BACKBONE_FEATURES).to(DEVICE)\n",
        "\n",
        "run_timestamp = time.time()\n",
        "SAVE_PATH = Path(f\"./results/{run_timestamp}/{class_name}\")\n",
        "\n",
        "train_dataset = MVTecDataset(DATA_PATH, class_name=class_name, is_train=True, resize=RESIZE, cropsize=CROP_SIZE)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
        "test_dataset = MVTecDataset(DATA_PATH, class_name=class_name, is_train=False, resize=RESIZE, cropsize=CROP_SIZE)\n",
        "val_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
        "\n",
        "\n",
        "padim_offline = PADIM(\n",
        "    backbone=BACKBONE,\n",
        "    device=DEVICE,\n",
        "    backbone_features_idx=indices,\n",
        "    save_path=SAVE_PATH,\n",
        "    plot_metrics=True,\n",
        ")\n",
        "padim_offline.train(train_dataloader)\n",
        "\n",
        "padim_online = PADIMWithOnlineCovariance(\n",
        "    backbone=BACKBONE,\n",
        "    device=DEVICE,\n",
        "    backbone_features_idx=indices,\n",
        "    save_path=SAVE_PATH,\n",
        "    plot_metrics=True,\n",
        ")\n",
        "padim_online.train(train_dataloader, NUMBER_OF_BACKBONE_FEATURES, int(CROP_SIZE/4), int(CROP_SIZE/4))\n",
        "\n",
        "torch.allclose(padim_online.mean, torch.Tensor(padim_offline.mean).to(DEVICE), atol=0.01) and torch.allclose(padim_online.cov, torch.Tensor(padim_offline.cov).to(DEVICE), atol=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQEY2gVfVZxx"
      },
      "source": [
        "#### 2.3 Performance experiments (10%)\n",
        "If you completed task 2.2, design experiments to empirically compare `space/memory` performance of PADIM training with both traditional and online covariance matrix estimation. Write short conclusions.\n",
        "\n",
        "#### 2.4 Bonus task (optional)\n",
        "You can also add similar experiments with conclusions with regard to the `time` complexity. This task is optional, but if you'll loose points elsewhere, this can help you to make up for some of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYH9l98tVZxy"
      },
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKuh7U-oVZxy"
      },
      "source": [
        "```Your conclusions go here```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml-teaching",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
